Epoch,Train Loss,Trian Time,Valid Loss,Valid Time,Eval Metrics,Test Loss,Test Metrics,Test Time,Time,Test Result Dir
1,111.41368842124939,0:00:01,12.136458098888397,0:00:00,"{'f1': 0.22569832402234638, 'precision': 0.2718707940780619, 'accuracy': 0.6847133757961783, 'roc_auc_score': 0.5156957143830871}",27.593783795833588,"{'f1': 0.19031055900621122, 'precision': 0.24042686754551162, 'accuracy': 0.6822657697182412, 'roc_auc_score': 0.501425482386077}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
2,101.06522053480148,0:00:01,11.213880777359009,0:00:00,"{'f1': 0.16920943134535368, 'precision': 0.30886075949367087, 'accuracy': 0.7274795268425842, 'roc_auc_score': 0.5175032647416741}",25.382957100868225,"{'f1': 0.13472777606890188, 'precision': 0.2673992673992674, 'accuracy': 0.7257482694745052, 'roc_auc_score': 0.5066860129056667}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
3,92.88882195949554,0:00:01,10.475590944290161,0:00:00,"{'f1': 0.12519561815336466, 'precision': 0.3463203463203463, 'accuracy': 0.7456778889899909, 'roc_auc_score': 0.5156603505073433}",24.068317651748657,"{'f1': 0.10688591983556013, 'precision': 0.3203285420944558, 'accuracy': 0.7458321146534074, 'roc_auc_score': 0.5109222086766437}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
4,87.04349756240845,0:00:01,10.00339525938034,0:00:00,"{'f1': 0.08862876254180602, 'precision': 0.35570469798657717, 'accuracy': 0.7520473157415832, 'roc_auc_score': 0.5109777740892875}",22.802238821983337,"{'f1': 0.08925979680696662, 'precision': 0.37962962962962965, 'accuracy': 0.7552890708784245, 'roc_auc_score': 0.5124443784681352}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
5,83.00431501865387,0:00:01,9.713029205799103,0:00:00,"{'f1': 0.08326105810928014, 'precision': 0.4528301886792453, 'accuracy': 0.7595541401273885, 'roc_auc_score': 0.5142633348191864}",22.00808048248291,"{'f1': 0.07362885048835463, 'precision': 0.4260869565217391, 'accuracy': 0.7595788242176075, 'roc_auc_score': 0.5117135215234573}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
6,79.96758198738098,0:00:01,9.352682173252106,0:00:00,"{'f1': 0.06914893617021277, 'precision': 0.48148148148148145, 'accuracy': 0.7611464968152867, 'roc_auc_score': 0.5123541133178359}",21.348755419254303,"{'f1': 0.05551272166538165, 'precision': 0.4444444444444444, 'accuracy': 0.7611387345227649, 'roc_auc_score': 0.5090518328569026}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
7,77.53814935684204,0:00:01,9.161023050546646,0:00:00,"{'f1': 0.061041292639138246, 'precision': 0.5074626865671642, 'accuracy': 0.762056414922657, 'roc_auc_score': 0.5113100234057523}",20.9633991420269,"{'f1': 0.046929996089166995, 'precision': 0.48, 'accuracy': 0.7624061616457054, 'roc_auc_score': 0.5081821716832016}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
8,75.93401110172272,0:00:01,9.112343907356262,0:00:00,"{'f1': 0.05967450271247739, 'precision': 0.559322033898305, 'accuracy': 0.7634212920837125, 'roc_auc_score': 0.5118775565729323}",20.63374462723732,"{'f1': 0.0440251572327044, 'precision': 0.5, 'accuracy': 0.7628936336160671, 'roc_auc_score': 0.5079348831343535}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
9,74.54447895288467,0:00:01,9.0215764939785,0:00:00,"{'f1': 0.05631244323342416, 'precision': 0.5740740740740741, 'accuracy': 0.763648771610555, 'roc_auc_score': 0.5113703416292994}",20.270510762929916,"{'f1': 0.04022082018927445, 'precision': 0.49038461538461536, 'accuracy': 0.7626986448279224, 'roc_auc_score': 0.5070986158987725}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
10,74.14431571960449,0:00:01,8.881358653306961,0:00:00,"{'f1': 0.05631244323342416, 'precision': 0.5740740740740741, 'accuracy': 0.763648771610555, 'roc_auc_score': 0.5113703416292994}",20.240377485752106,"{'f1': 0.03947887879984209, 'precision': 0.49504950495049505, 'accuracy': 0.7627961392219947, 'roc_auc_score': 0.5070208193206658}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
11,73.37691748142242,0:00:01,8.818521410226822,0:00:00,"{'f1': 0.05631244323342416, 'precision': 0.5740740740740741, 'accuracy': 0.763648771610555, 'roc_auc_score': 0.5113703416292994}",20.240169763565063,"{'f1': 0.03949447077409163, 'precision': 0.5, 'accuracy': 0.7628936336160671, 'roc_auc_score': 0.5070847170842442}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
12,73.66592669487,0:00:01,8.872579395771027,0:00:00,"{'f1': 0.05631244323342416, 'precision': 0.5740740740740741, 'accuracy': 0.763648771610555, 'roc_auc_score': 0.5113703416292994}",20.219775080680847,"{'f1': 0.039510075069142635, 'precision': 0.5050505050505051, 'accuracy': 0.7629911280101395, 'roc_auc_score': 0.5071486148478224}",0:00:00,20220125_1806,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_afm/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss

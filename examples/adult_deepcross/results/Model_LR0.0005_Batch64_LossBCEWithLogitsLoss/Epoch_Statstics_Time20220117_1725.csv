Epoch,Train Loss,Trian Time,Valid Loss,Valid Time,Eval Metrics,Test Loss,Test Metrics,Test Time,Time,Test Result Dir
1,260.3601580262184,0:00:03,28.30760730803013,0:00:00,"{'f1': 0.09172661870503596, 'precision': 0.7846153846153846, 'accuracy': 0.77024567788899, 'roc_auc_score': 0.5222651246876071}",65.80175307393074,"{'f1': 0.08368849283223556, 'precision': 0.7248322147651006, 'accuracy': 0.7694257580189139, 'roc_auc_score': 0.5195841390617117}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
2,223.82251200079918,0:00:02,26.450552493333817,0:00:00,"{'f1': 0.2779984114376489, 'precision': 0.8254716981132075, 'accuracy': 0.793221110100091, 'roc_auc_score': 0.5780480737667633}",61.40232764184475,"{'f1': 0.26368330464716006, 'precision': 0.8097251585623678, 'accuracy': 0.791459491079263, 'roc_auc_score': 0.5729909775937447}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
3,214.98265597224236,0:00:02,25.52671045064926,0:00:00,"{'f1': 0.40706713780918724, 'precision': 0.782608695652174, 'accuracy': 0.8091446769790719, 'roc_auc_score': 0.6255919527789588}",59.243211314082146,"{'f1': 0.42409638554216866, 'precision': 0.7927927927927928, 'accuracy': 0.8135907185336844, 'roc_auc_score': 0.6329796536068606}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
4,209.94690369069576,0:00:02,25.10821059346199,0:00:00,"{'f1': 0.4014285714285714, 'precision': 0.7960339943342776, 'accuracy': 0.8093721565059144, 'roc_auc_score': 0.6234434547312445}",58.15359964966774,"{'f1': 0.40523273501673257, 'precision': 0.7789473684210526, 'accuracy': 0.8093984595885737, 'roc_auc_score': 0.6248476647889692}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
5,205.91950884461403,0:00:02,24.84913568198681,0:00:00,"{'f1': 0.3323170731707316, 'precision': 0.8226415094339623, 'accuracy': 0.8007279344858963, 'roc_auc_score': 0.5970899522958427}",57.73657962679863,"{'f1': 0.36084452975047987, 'precision': 0.8126801152737753, 'accuracy': 0.805206200643463, 'roc_auc_score': 0.6076472381032453}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
6,202.85186405479908,0:00:02,24.516574501991272,0:00:00,"{'f1': 0.3982683982683982, 'precision': 0.8141592920353983, 'accuracy': 0.8102820746132848, 'roc_auc_score': 0.6223993648191608}",56.86849573254585,"{'f1': 0.4089242053789731, 'precision': 0.7964285714285714, 'accuracy': 0.8114458418640929, 'roc_auc_score': 0.6266146008491676}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
7,202.02007043361664,0:00:02,24.357300132513046,0:00:00,"{'f1': 0.409282700421941, 'precision': 0.776, 'accuracy': 0.8089171974522293, 'roc_auc_score': 0.6264274243434084}",56.22746105492115,"{'f1': 0.44792899408284026, 'precision': 0.7985232067510548, 'accuracy': 0.818075460661012, 'roc_auc_score': 0.64342875084076}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
8,200.15113528072834,0:00:02,24.14423866569996,0:00:00,"{'f1': 0.45967741935483875, 'precision': 0.7755102040816326, 'accuracy': 0.8171064604185623, 'roc_auc_score': 0.6485432507330162}",56.21822264790535,"{'f1': 0.4981787615578594, 'precision': 0.781882145998241, 'accuracy': 0.8253875402164376, 'roc_auc_score': 0.6669247362115351}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
9,198.58667029440403,0:00:02,24.16225056350231,0:00:00,"{'f1': 0.44322092222986925, 'precision': 0.7931034482758621, 'accuracy': 0.8159690627843494, 'roc_auc_score': 0.6412316268266939}",56.12205171585083,"{'f1': 0.48135398737808377, 'precision': 0.7960151802656547, 'accuracy': 0.8237301355172078, 'roc_auc_score': 0.6587537571464603}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
10,197.5831368714571,0:00:02,24.06542295217514,0:00:00,"{'f1': 0.49967721110393803, 'precision': 0.7709163346613546, 'accuracy': 0.8237033666969973, 'roc_auc_score': 0.6676444493117305}",55.49596916139126,"{'f1': 0.5250544662309368, 'precision': 0.7774193548387097, 'accuracy': 0.8299697767378376, 'roc_auc_score': 0.6805550067260804}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
11,196.83696223795414,0:00:02,24.15658140182495,0:00:00,"{'f1': 0.46029609690444145, 'precision': 0.7790432801822323, 'accuracy': 0.8175614194722475, 'roc_auc_score': 0.648841847329015}",55.68587650358677,"{'f1': 0.49830890642615555, 'precision': 0.7921146953405018, 'accuracy': 0.8264599785512333, 'roc_auc_score': 0.6669191399024719}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
12,197.0615932494402,0:00:02,24.118812009692192,0:00:00,"{'f1': 0.5099806825499034, 'precision': 0.782608695652174, 'accuracy': 0.8268880800727935, 'roc_auc_score': 0.6726889350710686}",55.565729051828384,"{'f1': 0.5315975047464062, 'precision': 0.7808764940239044, 'accuracy': 0.8316271814370674, 'roc_auc_score': 0.6839083781738692}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
13,195.4114474505186,0:00:02,23.977761283516884,0:00:00,"{'f1': 0.5147247119078106, 'precision': 0.7805825242718447, 'accuracy': 0.8275705186533212, 'roc_auc_score': 0.6751063696899644}",55.32779698073864,"{'f1': 0.540177371674281, 'precision': 0.7796741660201707, 'accuracy': 0.8331870917422248, 'roc_auc_score': 0.6884731009332437}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
14,195.0206753909588,0:00:02,24.063207656145096,0:00:00,"{'f1': 0.4854881266490765, 'precision': 0.7846481876332623, 'accuracy': 0.8225659690627843, 'roc_auc_score': 0.6606610820262246}",55.151240438222885,"{'f1': 0.5217391304347826, 'precision': 0.7886855241264559, 'accuracy': 0.8305547431022716, 'roc_auc_score': 0.6786712838405919}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
15,194.3187728971243,0:00:02,24.03875309228897,0:00:00,"{'f1': 0.4642375168690958, 'precision': 0.7908045977011494, 'accuracy': 0.8193812556869882, 'roc_auc_score': 0.6506927469546427}",55.24399934709072,"{'f1': 0.4961767204757859, 'precision': 0.7970882620564149, 'accuracy': 0.8265574729453057, 'roc_auc_score': 0.6658494829325711}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
16,194.79350416362286,0:00:02,23.924119770526886,0:00:00,"{'f1': 0.5153061224489797, 'precision': 0.7754318618042226, 'accuracy': 0.827115559599636, 'roc_auc_score': 0.6754642863355981}",55.06190188229084,"{'f1': 0.535598705501618, 'precision': 0.7782131661442007, 'accuracy': 0.8321146534074291, 'roc_auc_score': 0.6860698934336641}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
17,192.87926329672337,0:00:02,23.947930932044983,0:00:00,"{'f1': 0.5130823229100191, 'precision': 0.7730769230769231, 'accuracy': 0.8264331210191083, 'roc_auc_score': 0.6743598781999675}",55.246881082654,"{'f1': 0.5401342281879195, 'precision': 0.7780355761794276, 'accuracy': 0.8329921029540801, 'roc_auc_score': 0.688486999747772}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
18,193.1853547990322,0:00:02,23.989816680550575,0:00:00,"{'f1': 0.5045045045045046, 'precision': 0.7731755424063116, 'accuracy': 0.8248407643312102, 'roc_auc_score': 0.6700322239058089}",54.929121389985085,"{'f1': 0.5396825396825397, 'precision': 0.7805447470817121, 'accuracy': 0.8331870917422248, 'roc_auc_score': 0.6881897122498738}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
19,193.77757665514946,0:00:02,24.0346472710371,0:00:00,"{'f1': 0.5485232067510547, 'precision': 0.7434640522875817, 'accuracy': 0.8296178343949044, 'roc_auc_score': 0.6938476552752209}",55.41875322163105,"{'f1': 0.5739887352790578, 'precision': 0.7605156037991859, 'accuracy': 0.8377693282636248, 'roc_auc_score': 0.707912839456869}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
20,193.57966348528862,0:00:02,23.891338527202606,0:00:00,"{'f1': 0.5080179602309173, 'precision': 0.7734375, 'accuracy': 0.825523202911738, 'roc_auc_score': 0.6717931452830722}",55.10473911464214,"{'f1': 0.5357817985417229, 'precision': 0.7804878048780488, 'accuracy': 0.832407136589646, 'roc_auc_score': 0.686119892382714}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
21,193.1577558964491,0:00:02,23.862678855657578,0:00:00,"{'f1': 0.5067350865939705, 'precision': 0.771484375, 'accuracy': 0.8250682438580528, 'roc_auc_score': 0.6711662920662571}",54.94880302250385,"{'f1': 0.5327558202490525, 'precision': 0.7797147385103012, 'accuracy': 0.8317246758311397, 'roc_auc_score': 0.684539053304187}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
22,191.66884770989418,0:00:02,23.841195195913315,0:00:00,"{'f1': 0.5324189526184538, 'precision': 0.7666068222621185, 'accuracy': 0.8293903548680619, 'roc_auc_score': 0.684507171594366}",54.83538606762886,"{'f1': 0.5536753040719197, 'precision': 0.7755555555555556, 'accuracy': 0.8354294628058887, 'roc_auc_score': 0.695893911846309}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
23,191.9806226938963,0:00:02,23.812695130705833,0:00:00,"{'f1': 0.540272614622057, 'precision': 0.7689594356261023, 'accuracy': 0.8312101910828026, 'roc_auc_score': 0.6886558675657076}",54.89343377947807,"{'f1': 0.553943550514376, 'precision': 0.7726269315673289, 'accuracy': 0.8351369796236716, 'roc_auc_score': 0.6961273015806289}",0:00:00,20220117_1721,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
24,192.25900873541832,0:00:02,23.883751422166824,0:00:00,"{'f1': 0.5108834827144687, 'precision': 0.7747572815533981, 'accuracy': 0.8262056414922657, 'roc_auc_score': 0.6732258100395191}",54.83028908073902,"{'f1': 0.5369272237196766, 'precision': 0.7793427230046949, 'accuracy': 0.8325046309837184, 'roc_auc_score': 0.6867505675130319}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
25,191.67187605798244,0:00:02,23.849051862955093,0:00:00,"{'f1': 0.5363748458692972, 'precision': 0.7565217391304347, 'accuracy': 0.8289353958143767, 'roc_auc_score': 0.6868346279648974}",54.758168175816536,"{'f1': 0.563314226159458, 'precision': 0.7688477951635846, 'accuracy': 0.8365993955347567, 'roc_auc_score': 0.7014782926265344}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
26,192.84619636833668,0:00:02,23.942232698202133,0:00:00,"{'f1': 0.5536038764385222, 'precision': 0.7566225165562914, 'accuracy': 0.8323475887170154, 'roc_auc_score': 0.6962957480928461}",55.18197473883629,"{'f1': 0.5749360613810742, 'precision': 0.7604871447902571, 'accuracy': 0.8379643170517695, 'roc_auc_score': 0.7084657180090802}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
27,190.62170791625977,0:00:02,23.819016456604004,0:00:00,"{'f1': 0.506426735218509, 'precision': 0.7740667976424361, 'accuracy': 0.8252957233848953, 'roc_auc_score': 0.6709873337434402}",54.51587292551994,"{'f1': 0.5332609285908226, 'precision': 0.7849720223820943, 'accuracy': 0.832407136589646, 'roc_auc_score': 0.684702948965865}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
28,191.11279962956905,0:00:02,23.7868342846632,0:00:00,"{'f1': 0.5392702535559679, 'precision': 0.7649122807017544, 'accuracy': 0.8305277525022748, 'roc_auc_score': 0.6882079726717094}",55.00919908285141,"{'f1': 0.5566037735849058, 'precision': 0.7673410404624278, 'accuracy': 0.8350394852295993, 'roc_auc_score': 0.6977637359172691}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
29,191.8512065410614,0:00:02,23.90269862115383,0:00:00,"{'f1': 0.5338299193047796, 'precision': 0.7624113475177305, 'accuracy': 0.8291628753412192, 'roc_auc_score': 0.6853426431588154}",54.812376245856285,"{'f1': 0.5616688396349413, 'precision': 0.7676407697790449, 'accuracy': 0.836111923564395, 'roc_auc_score': 0.7005920264419034}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
30,190.5255982875824,0:00:02,23.942363277077675,0:00:00,"{'f1': 0.5642857142857143, 'precision': 0.7488151658767772, 'accuracy': 0.8334849863512284, 'roc_auc_score': 0.7026226021367196}",54.83379705250263,"{'f1': 0.5875820292781423, 'precision': 0.7607843137254902, 'accuracy': 0.8406941600857951, 'roc_auc_score': 0.7159226290566673}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
31,191.50951066613197,0:00:02,23.915965616703033,0:00:00,"{'f1': 0.47904191616766467, 'precision': 0.7894736842105263, 'accuracy': 0.8218835304822566, 'roc_auc_score': 0.6575871341656964}",54.813869670033455,"{'f1': 0.5093653899916131, 'precision': 0.7956331877729258, 'accuracy': 0.8288973384030418, 'roc_auc_score': 0.6723423312174205}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
32,190.51049642264843,0:00:02,23.774313047528267,0:00:00,"{'f1': 0.5029239766081872, 'precision': 0.7865853658536586, 'accuracy': 0.8259781619654231, 'roc_auc_score': 0.6691374322917246}",54.77606971561909,"{'f1': 0.5283533260632497, 'precision': 0.7839805825242718, 'accuracy': 0.8313346982548504, 'roc_auc_score': 0.6821580471246007}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
33,191.82961812615395,0:00:02,23.77987051010132,0:00:00,"{'f1': 0.5061488673139158, 'precision': 0.785140562248996, 'accuracy': 0.8264331210191083, 'roc_auc_score': 0.6707490553709885}",54.61514404416084,"{'f1': 0.5357529794149513, 'precision': 0.7849206349206349, 'accuracy': 0.8328946085600079, 'roc_auc_score': 0.6860142981755507}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
34,190.11989770829678,0:00:02,23.727610036730766,0:00:00,"{'f1': 0.5319014529374605, 'precision': 0.7854477611940298, 'accuracy': 0.8314376706096451, 'roc_auc_score': 0.6838813165514631}",54.72373057901859,"{'f1': 0.5472051350628511, 'precision': 0.782708492731446, 'accuracy': 0.834941990835527, 'roc_auc_score': 0.6921737588279805}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
35,190.0117960423231,0:00:02,23.71019507944584,0:00:00,"{'f1': 0.5312302839116719, 'precision': 0.7825278810408922, 'accuracy': 0.8309827115559599, 'roc_auc_score': 0.6835827199554643}",54.59940046072006,"{'f1': 0.5464247598719316, 'precision': 0.7781155015197568, 'accuracy': 0.8342595300770206, 'roc_auc_score': 0.6918681688246174}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
36,190.1011783182621,0:00:02,23.984036058187485,0:00:00,"{'f1': 0.5649038461538461, 'precision': 0.7617504051863857, 'accuracy': 0.835304822565969, 'roc_auc_score': 0.7025039620374498}",55.24905654788017,"{'f1': 0.5815602836879432, 'precision': 0.7572559366754618, 'accuracy': 0.8389392609924929, 'roc_auc_score': 0.7125053598453002}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
37,190.0540383309126,0:00:02,23.742733865976334,0:00:00,"{'f1': 0.5159642401021712, 'precision': 0.7784200385356455, 'accuracy': 0.8275705186533212, 'roc_auc_score': 0.6757628829315968}",54.5707046687603,"{'f1': 0.5432164838105432, 'precision': 0.7777777777777778, 'accuracy': 0.8335770693185142, 'roc_auc_score': 0.6901456354044058}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
38,190.00120839476585,0:00:03,23.741441428661346,0:00:00,"{'f1': 0.5252525252525252, 'precision': 0.7746741154562383, 'accuracy': 0.8289353958143767, 'roc_auc_score': 0.6805977521693884}",54.490480318665504,"{'f1': 0.5467164975974372, 'precision': 0.7792998477929984, 'accuracy': 0.8344545188651653, 'roc_auc_score': 0.691995964351774}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
39,190.899105027318,0:00:02,23.70167861878872,0:00:00,"{'f1': 0.5355596784168213, 'precision': 0.7596491228070176, 'accuracy': 0.8291628753412192, 'roc_auc_score': 0.6863274130212642}",54.53480540215969,"{'f1': 0.5659397715472483, 'precision': 0.7676056338028169, 'accuracy': 0.8369893731110462, 'roc_auc_score': 0.7030091327560115}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
40,190.45036858320236,0:00:02,23.66694785654545,0:00:00,"{'f1': 0.5288220551378446, 'precision': 0.7686703096539163, 'accuracy': 0.8289353958143767, 'roc_auc_score': 0.682567291894286}",54.474185526371,"{'f1': 0.553968253968254, 'precision': 0.776706231454006, 'accuracy': 0.8356244515940333, 'roc_auc_score': 0.6960217073734656}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
41,189.79125817120075,0:00:02,23.711596801877022,0:00:00,"{'f1': 0.5243209096651926, 'precision': 0.7742537313432836, 'accuracy': 0.8287079162875342, 'roc_auc_score': 0.6801201972505727}",54.44550895690918,"{'f1': 0.5487576810045418, 'precision': 0.7833714721586575, 'accuracy': 0.8353319684118163, 'roc_auc_score': 0.6929961272490331}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
42,189.7690944224596,0:00:02,23.728260174393654,0:00:00,"{'f1': 0.5163776493256262, 'precision': 0.788235294117647, 'accuracy': 0.8287079162875342, 'roc_auc_score': 0.6758528611799615}",54.46766701340675,"{'f1': 0.5331161780673181, 'precision': 0.7843450479233227, 'accuracy': 0.8323096421955738, 'roc_auc_score': 0.6846390512022869}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
43,189.4665071517229,0:00:02,23.78364449739456,0:00:00,"{'f1': 0.5457897971727105, 'precision': 0.7655172413793103, 'accuracy': 0.8318926296633303, 'roc_auc_score': 0.6917298154262359}",54.40329723060131,"{'f1': 0.5658307210031348, 'precision': 0.7757879656160458, 'accuracy': 0.8379643170517695, 'roc_auc_score': 0.70265625}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
44,190.85967560112476,0:00:02,23.70934945344925,0:00:00,"{'f1': 0.5265822784810127, 'precision': 0.7804878048780488, 'accuracy': 0.8298453139217471, 'roc_auc_score': 0.681194945361386}",54.58305202424526,"{'f1': 0.5471092077087796, 'precision': 0.7837423312883436, 'accuracy': 0.8350394852295993, 'roc_auc_score': 0.6920959622498739}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
45,190.33002032339573,0:00:02,23.686670035123825,0:00:00,"{'f1': 0.5311516677155443, 'precision': 0.7785977859778598, 'accuracy': 0.8305277525022748, 'roc_auc_score': 0.6836123799802817}",54.74867323040962,"{'f1': 0.5531689207106869, 'precision': 0.7789395070948469, 'accuracy': 0.8357219459881057, 'roc_auc_score': 0.6955188277703044}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
46,189.93406668305397,0:00:02,23.726681292057037,0:00:00,"{'f1': 0.5401730531520396, 'precision': 0.7653239929947461, 'accuracy': 0.8307552320291174, 'roc_auc_score': 0.688685527590525}",54.70726038515568,"{'f1': 0.5615505500261917, 'precision': 0.7734487734487735, 'accuracy': 0.8367943843229014, 'roc_auc_score': 0.700330839078527}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
47,189.4132024049759,0:00:02,23.73542696237564,0:00:00,"{'f1': 0.5244444444444445, 'precision': 0.7821969696969697, 'accuracy': 0.8296178343949044, 'roc_auc_score': 0.6800608772009378}",54.43979272246361,"{'f1': 0.5473965287049399, 'precision': 0.7806549885757806, 'accuracy': 0.8347470020473823, 'roc_auc_score': 0.6923293519841938}",0:00:00,20220117_1722,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
48,188.67565198242664,0:00:02,23.69926545023918,0:00:00,"{'f1': 0.5517661388550548, 'precision': 0.761344537815126, 'accuracy': 0.8325750682438581, 'roc_auc_score': 0.6951320199075804}",54.58351545035839,"{'f1': 0.5771604938271604, 'precision': 0.7706043956043956, 'accuracy': 0.8397192161450716, 'roc_auc_score': 0.7093324890701194}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
49,189.71612618863583,0:00:02,23.79041889309883,0:00:00,"{'f1': 0.5551515151515151, 'precision': 0.7595356550580431, 'accuracy': 0.8330300272975433, 'roc_auc_score': 0.6970718996076605}",54.68154563009739,"{'f1': 0.5800865800865801, 'precision': 0.7618729096989967, 'accuracy': 0.8392317441747099, 'roc_auc_score': 0.7114218040608711}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
50,189.3109976798296,0:00:02,23.677197352051735,0:00:00,"{'f1': 0.5473554735547356, 'precision': 0.768566493955095, 'accuracy': 0.8325750682438581, 'roc_auc_score': 0.6925059669410504}",54.28878290951252,"{'f1': 0.5684701009578048, 'precision': 0.7672955974842768, 'accuracy': 0.8374768450814079, 'roc_auc_score': 0.7044621763073818}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
51,189.1379699409008,0:00:02,23.663143932819366,0:00:00,"{'f1': 0.5293005671077505, 'precision': 0.7777777777777778, 'accuracy': 0.8300727934485896, 'roc_auc_score': 0.6826572701426505}",54.264576360583305,"{'f1': 0.5475427350427351, 'precision': 0.78125, 'accuracy': 0.8348444964414546, 'roc_auc_score': 0.692393249747772}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
52,188.53156270086765,0:00:02,23.766232937574387,0:00:00,"{'f1': 0.5475751995089011, 'precision': 0.7663230240549829, 'accuracy': 0.8323475887170154, 'roc_auc_score': 0.6926849252638672}",54.56039659678936,"{'f1': 0.5737874097007225, 'precision': 0.7700831024930748, 'accuracy': 0.8389392609924929, 'roc_auc_score': 0.7074043635446444}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
53,189.57406789064407,0:00:02,23.758693531155586,0:00:00,"{'f1': 0.5437731196054254, 'precision': 0.7669565217391304, 'accuracy': 0.8316651501364877, 'roc_auc_score': 0.6905957472657877}",54.45714417099953,"{'f1': 0.5657826313052523, 'precision': 0.7694483734087695, 'accuracy': 0.8371843618991908, 'roc_auc_score': 0.7028535395997981}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
54,188.67561303079128,0:00:02,23.69082848727703,0:00:00,"{'f1': 0.5415117719950434, 'precision': 0.7707231040564374, 'accuracy': 0.8316651501364877, 'roc_auc_score': 0.6892827207825228}",54.44030798971653,"{'f1': 0.5623689727463314, 'precision': 0.7752890173410405, 'accuracy': 0.8371843618991908, 'roc_auc_score': 0.7007281244745249}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
55,188.2299988567829,0:00:02,23.766920819878578,0:00:00,"{'f1': 0.5602879424115177, 'precision': 0.7532258064516129, 'accuracy': 0.8332575068243858, 'roc_auc_score': 0.7001755074930064}",54.68011450767517,"{'f1': 0.5911602209944751, 'precision': 0.7593548387096775, 'accuracy': 0.8412791264502291, 'roc_auc_score': 0.7181480420800403}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
56,189.12767362594604,0:00:02,23.728424310684204,0:00:00,"{'f1': 0.5167095115681234, 'precision': 0.7897838899803536, 'accuracy': 0.8289353958143767, 'roc_auc_score': 0.6760021594779608}",54.40224891901016,"{'f1': 0.5368763557483731, 'precision': 0.7882165605095541, 'accuracy': 0.8334795749244418, 'roc_auc_score': 0.6865393790987052}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
57,189.0951539427042,0:00:02,23.743915155529976,0:00:00,"{'f1': 0.5534514355528405, 'precision': 0.7677966101694915, 'accuracy': 0.833712465878071, 'roc_auc_score': 0.6958785113975775}",54.33306805789471,"{'f1': 0.5778120184899845, 'precision': 0.7694938440492476, 'accuracy': 0.8397192161450716, 'roc_auc_score': 0.7097575720951741}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
58,189.63391716778278,0:00:02,23.738252207636833,0:00:00,"{'f1': 0.5236593059936909, 'precision': 0.7713754646840149, 'accuracy': 0.828252957233849, 'roc_auc_score': 0.6798216006545739}",54.42548552155495,"{'f1': 0.5487056311716041, 'precision': 0.7817490494296578, 'accuracy': 0.8351369796236716, 'roc_auc_score': 0.6930100260635614}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
59,188.6401108801365,0:00:02,23.691255897283554,0:00:00,"{'f1': 0.5284909204758923, 'precision': 0.7672727272727272, 'accuracy': 0.8287079162875342, 'roc_auc_score': 0.6824179935962865}",54.27378214895725,"{'f1': 0.5562005277044855, 'precision': 0.7761413843888071, 'accuracy': 0.8360144291703228, 'roc_auc_score': 0.6972691588195729}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
60,188.74997736513615,0:00:02,23.643484637141228,0:00:00,"{'f1': 0.5441085749537322, 'precision': 0.7682926829268293, 'accuracy': 0.8318926296633303, 'roc_auc_score': 0.6907450455637871}",54.39320731163025,"{'f1': 0.5651720542231491, 'precision': 0.7720797720797721, 'accuracy': 0.8373793506873355, 'roc_auc_score': 0.7024145577602152}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
61,188.60250081121922,0:00:02,23.72082780301571,0:00:00,"{'f1': 0.5395638629283489, 'precision': 0.7759856630824373, 'accuracy': 0.8318926296633303, 'roc_auc_score': 0.6881189925972571}",54.403652757406235,"{'f1': 0.5621877465159085, 'precision': 0.7797228300510576, 'accuracy': 0.8376718338695525, 'roc_auc_score': 0.7004808359256768}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
62,189.08828482031822,0:00:02,23.742832958698273,0:00:00,"{'f1': 0.529968454258675, 'precision': 0.7806691449814126, 'accuracy': 0.8305277525022748, 'roc_auc_score': 0.6829558667386493}",54.447974875569344,"{'f1': 0.5504391801969657, 'precision': 0.780377358490566, 'accuracy': 0.8353319684118163, 'roc_auc_score': 0.6939879876408274}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
63,188.35643653571606,0:00:02,23.726546451449394,0:00:00,"{'f1': 0.5162939297124601, 'precision': 0.7799227799227799, 'accuracy': 0.8277979981801638, 'roc_auc_score': 0.6759121812295963}",54.477080911397934,"{'f1': 0.5432032301480485, 'precision': 0.7864380358534684, 'accuracy': 0.8345520132592376, 'roc_auc_score': 0.6899344469900791}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
64,187.33467096090317,0:00:02,23.717311054468155,0:00:00,"{'f1': 0.5185185185185185, 'precision': 0.7822736030828517, 'accuracy': 0.8284804367606915, 'roc_auc_score': 0.677016589365227}",54.38798761367798,"{'f1': 0.5375472717450028, 'precision': 0.7834645669291339, 'accuracy': 0.8330895973481525, 'roc_auc_score': 0.6869922597528165}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
65,188.77910433709621,0:00:02,23.648947596549988,0:00:00,"{'f1': 0.5596608116293156, 'precision': 0.7649006622516556, 'accuracy': 0.8346223839854413, 'roc_auc_score': 0.6994300141769215}",54.494251906871796,"{'f1': 0.5788667687595712, 'precision': 0.7631224764468372, 'accuracy': 0.8391342497806377, 'roc_auc_score': 0.7106494345888683}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
66,187.7639228850603,0:00:02,23.69345012307167,0:00:00,"{'f1': 0.5209656925031766, 'precision': 0.777988614800759, 'accuracy': 0.8284804367606915, 'roc_auc_score': 0.678329615848492}",54.370425537228584,"{'f1': 0.5465738758029978, 'precision': 0.7829754601226994, 'accuracy': 0.8348444964414546, 'roc_auc_score': 0.6918264723810325}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
67,187.97764129936695,0:00:02,23.735758736729622,0:00:00,"{'f1': 0.5304839723444374, 'precision': 0.7757352941176471, 'accuracy': 0.8300727934485896, 'roc_auc_score': 0.683313783384283}",54.41677665710449,"{'f1': 0.5518705226850623, 'precision': 0.7778608825729244, 'accuracy': 0.8353319684118163, 'roc_auc_score': 0.6948381536909366}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
68,188.4687314927578,0:00:02,23.675263538956642,0:00:00,"{'f1': 0.5252525252525252, 'precision': 0.7746741154562383, 'accuracy': 0.8289353958143767, 'roc_auc_score': 0.6805977521693884}",54.408630803227425,"{'f1': 0.5537234042553191, 'precision': 0.7838855421686747, 'accuracy': 0.8364044067466121, 'roc_auc_score': 0.6956827234319826}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
69,187.37014327943325,0:00:02,23.69910518825054,0:00:00,"{'f1': 0.553113553113553, 'precision': 0.766497461928934, 'accuracy': 0.8334849863512284, 'roc_auc_score': 0.6957292130995781}",54.430880039930344,"{'f1': 0.5765580918184149, 'precision': 0.7661895023858214, 'accuracy': 0.8390367553865653, 'roc_auc_score': 0.7091685934084413}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
70,188.14847077429295,0:00:02,23.7237538844347,0:00:00,"{'f1': 0.537647790914748, 'precision': 0.7714285714285715, 'accuracy': 0.8309827115559599, 'roc_auc_score': 0.6871935427844432}",54.283608704805374,"{'f1': 0.5621565035331065, 'precision': 0.7732181425485961, 'accuracy': 0.8368918787169738, 'roc_auc_score': 0.7006781255254751}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
71,187.13549309968948,0:00:02,23.65353736281395,0:00:00,"{'f1': 0.5565006075334143, 'precision': 0.7646076794657763, 'accuracy': 0.8339399454049136, 'roc_auc_score': 0.6976690927996583}",54.351383715867996,"{'f1': 0.5758974358974359, 'precision': 0.7649863760217984, 'accuracy': 0.8387442722043482, 'roc_auc_score': 0.7088352057760215}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
72,188.1589917242527,0:00:02,23.678272560238838,0:00:00,"{'f1': 0.5311516677155443, 'precision': 0.7785977859778598, 'accuracy': 0.8305277525022748, 'roc_auc_score': 0.6836123799802817}",54.2526028752327,"{'f1': 0.5537321334039174, 'precision': 0.7771173848439822, 'accuracy': 0.8356244515940333, 'roc_auc_score': 0.6958800130317807}",0:00:00,20220117_1723,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
73,188.31724245846272,0:00:02,23.716647282242775,0:00:00,"{'f1': 0.5480295566502463, 'precision': 0.7712305025996534, 'accuracy': 0.8330300272975433, 'roc_auc_score': 0.6928045635370492}",54.25837557017803,"{'f1': 0.5729785585120124, 'precision': 0.7706740792216817, 'accuracy': 0.8388417665984206, 'roc_auc_score': 0.7069153827560115}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
74,188.0347098261118,0:00:02,23.65280757844448,0:00:00,"{'f1': 0.536403235843186, 'precision': 0.7696428571428572, 'accuracy': 0.8305277525022748, 'roc_auc_score': 0.6865666895676281}",54.24774640798569,"{'f1': 0.5657516339869282, 'precision': 0.7767408470926059, 'accuracy': 0.8380618114458419, 'roc_auc_score': 0.7025784534218934}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
75,188.39040507376194,0:00:02,23.798273772001266,0:00:00,"{'f1': 0.5041800643086817, 'precision': 0.7716535433070866, 'accuracy': 0.8246132848043676, 'roc_auc_score': 0.6698829256078096}",54.37796272337437,"{'f1': 0.5408906882591094, 'precision': 0.7871170463472114, 'accuracy': 0.8341620356829482, 'roc_auc_score': 0.6886869955439717}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
76,188.0418877005577,0:00:02,23.68852373957634,0:00:00,"{'f1': 0.5356695869837296, 'precision': 0.7767695099818511, 'accuracy': 0.8312101910828026, 'roc_auc_score': 0.6860298145991776}",54.593844667077065,"{'f1': 0.5602743339488262, 'precision': 0.7814569536423841, 'accuracy': 0.8374768450814079, 'roc_auc_score': 0.6993611800067261}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
77,188.71497875452042,0:00:02,23.682737916707993,0:00:00,"{'f1': 0.5494775660725261, 'precision': 0.7706896551724138, 'accuracy': 0.8332575068243858, 'roc_auc_score': 0.6936103750766812}",54.3365425914526,"{'f1': 0.573502722323049, 'precision': 0.776140350877193, 'accuracy': 0.8396217217509994, 'roc_auc_score': 0.707001481839583}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
78,188.37666107714176,0:00:02,23.629860043525696,0:00:00,"{'f1': 0.5459026494146642, 'precision': 0.7690972222222222, 'accuracy': 0.8323475887170154, 'roc_auc_score': 0.6917001554014185}",54.323174089193344,"{'f1': 0.5705394190871369, 'precision': 0.7724719101123596, 'accuracy': 0.8385492834162036, 'roc_auc_score': 0.7054484403901127}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
79,188.047953248024,0:00:02,23.71963034570217,0:00:00,"{'f1': 0.5283018867924529, 'precision': 0.7734806629834254, 'accuracy': 0.8293903548680619, 'roc_auc_score': 0.6822093752486522}",54.45093834400177,"{'f1': 0.5573770491803278, 'precision': 0.7807407407407407, 'accuracy': 0.8367943843229014, 'roc_auc_score': 0.697780340928199}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
80,187.3112843632698,0:00:02,23.722202226519585,0:00:00,"{'f1': 0.5503685503685505, 'precision': 0.7710843373493976, 'accuracy': 0.8334849863512284, 'roc_auc_score': 0.6940879299954968}",54.21736867725849,"{'f1': 0.5765069551777433, 'precision': 0.7717241379310344, 'accuracy': 0.8397192161450716, 'roc_auc_score': 0.7089074060450647}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
81,188.36378049850464,0:00:02,23.666084975004196,0:00:00,"{'f1': 0.5334166146158651, 'precision': 0.7707581227436823, 'accuracy': 0.8300727934485896, 'roc_auc_score': 0.6849550664883644}",54.28482185304165,"{'f1': 0.5615324061926004, 'precision': 0.7759245830311821, 'accuracy': 0.8370868675051184, 'roc_auc_score': 0.700239143685892}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
82,187.7065685093403,0:00:02,23.686636805534363,0:00:00,"{'f1': 0.544780728844966, 'precision': 0.7709790209790209, 'accuracy': 0.8323475887170154, 'roc_auc_score': 0.691043642159786}",54.265125036239624,"{'f1': 0.5716506867064005, 'precision': 0.7729502452697968, 'accuracy': 0.8388417665984206, 'roc_auc_score': 0.7060652167059022}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
83,187.6832531094551,0:00:02,23.681654572486877,0:00:00,"{'f1': 0.5602409638554217, 'precision': 0.7585644371941273, 'accuracy': 0.8339399454049136, 'roc_auc_score': 0.699966889145372}",54.25671799480915,"{'f1': 0.5841959101237062, 'precision': 0.7567037279267496, 'accuracy': 0.8394267329628546, 'roc_auc_score': 0.7141000977383555}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
84,187.67478600144386,0:00:02,23.67025053501129,0:00:00,"{'f1': 0.5277078085642317, 'precision': 0.7744916820702403, 'accuracy': 0.8293903548680619, 'roc_auc_score': 0.681881118627836}",54.12125891447067,"{'f1': 0.5539606592238171, 'precision': 0.7834586466165413, 'accuracy': 0.8364044067466121, 'roc_auc_score': 0.6958244177736674}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
85,188.01153588294983,0:00:02,23.662059545516968,0:00:00,"{'f1': 0.5401369010578718, 'precision': 0.775, 'accuracy': 0.8318926296633303, 'roc_auc_score': 0.6884472492180733}",54.18129904568195,"{'f1': 0.564625850340136, 'precision': 0.7762589928057554, 'accuracy': 0.8377693282636248, 'roc_auc_score': 0.7019616771061039}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
86,187.69704404473305,0:00:02,23.654533848166466,0:00:00,"{'f1': 0.5380774032459424, 'precision': 0.7765765765765765, 'accuracy': 0.8316651501364877, 'roc_auc_score': 0.6873131810576251}",54.20904541015625,"{'f1': 0.559642669469259, 'precision': 0.7751091703056768, 'accuracy': 0.8365993955347567, 'roc_auc_score': 0.6992111831595762}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
87,187.10567803680897,0:00:02,23.646155953407288,0:00:00,"{'f1': 0.5301507537688442, 'precision': 0.7743119266055046, 'accuracy': 0.8298453139217471, 'roc_auc_score': 0.6831644850862836}",54.16473354399204,"{'f1': 0.556553911205074, 'precision': 0.7788461538461539, 'accuracy': 0.8364044067466121, 'roc_auc_score': 0.6973830555322011}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
88,187.07011020183563,0:00:02,23.63109016418457,0:00:00,"{'f1': 0.558983666061706, 'precision': 0.7623762376237624, 'accuracy': 0.8341674249317561, 'roc_auc_score': 0.6991314175809227}",54.49182939529419,"{'f1': 0.5852546237648848, 'precision': 0.7623762376237624, 'accuracy': 0.840401676903578, 'roc_auc_score': 0.7144556866907684}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
89,187.44879573583603,0:00:02,23.67267993092537,0:00:00,"{'f1': 0.53375, 'precision': 0.7721518987341772, 'accuracy': 0.8303002729754322, 'roc_auc_score': 0.6851043647863637}",54.15427753329277,"{'f1': 0.5605263157894737, 'precision': 0.7785087719298246, 'accuracy': 0.8371843618991908, 'roc_auc_score': 0.6995945697410458}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
90,187.18616278469563,0:00:02,23.650621965527534,0:00:00,"{'f1': 0.537647790914748, 'precision': 0.7714285714285715, 'accuracy': 0.8309827115559599, 'roc_auc_score': 0.6871935427844432}",54.079222455620766,"{'f1': 0.5632033499084009, 'precision': 0.7746580273578114, 'accuracy': 0.8372818562932631, 'roc_auc_score': 0.7012171052631578}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
91,187.63939878344536,0:00:02,23.67074389755726,0:00:00,"{'f1': 0.5565006075334143, 'precision': 0.7646076794657763, 'accuracy': 0.8339399454049136, 'roc_auc_score': 0.6976690927996583}",54.17220892012119,"{'f1': 0.5811878664287535, 'precision': 0.7645875251509054, 'accuracy': 0.839816710539144, 'roc_auc_score': 0.7119468849840256}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
92,187.78486108779907,0:00:02,23.64803145825863,0:00:00,"{'f1': 0.5391304347826087, 'precision': 0.7708703374777975, 'accuracy': 0.8312101910828026, 'roc_auc_score': 0.6879993543240751}",54.1920917481184,"{'f1': 0.5637126376507603, 'precision': 0.7778581765557163, 'accuracy': 0.8377693282636248, 'roc_auc_score': 0.7013948997393644}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
93,187.29270708560944,0:00:02,23.734038218855858,0:00:00,"{'f1': 0.5109114249037227, 'precision': 0.7788649706457925, 'accuracy': 0.8266606005459509, 'roc_auc_score': 0.6731961500147017}",54.33443610370159,"{'f1': 0.5410940447318782, 'precision': 0.7849882720875684, 'accuracy': 0.8339670468948035, 'roc_auc_score': 0.688842588700185}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
94,187.67287781834602,0:00:02,23.61916995048523,0:00:00,"{'f1': 0.5221238938053098, 'precision': 0.77196261682243, 'accuracy': 0.8280254777070064, 'roc_auc_score': 0.679015789114942}",54.42085883021355,"{'f1': 0.5554965480616039, 'precision': 0.7841079460269865, 'accuracy': 0.8367943843229014, 'roc_auc_score': 0.6966467861947201}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
95,188.65230536460876,0:00:02,23.615201756358147,0:00:00,"{'f1': 0.5412787088764743, 'precision': 0.7730496453900709, 'accuracy': 0.8318926296633303, 'roc_auc_score': 0.6891037624597058}",54.1422405987978,"{'f1': 0.5629746006808066, 'precision': 0.7750540735400144, 'accuracy': 0.8372818562932631, 'roc_auc_score': 0.701075410921473}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
96,187.56635065376759,0:00:02,23.623256891965866,0:00:00,"{'f1': 0.5316614420062696, 'precision': 0.7737226277372263, 'accuracy': 0.8300727934485896, 'roc_auc_score': 0.6839702966259156}",54.227368503808975,"{'f1': 0.560737812911726, 'precision': 0.780630961115187, 'accuracy': 0.8374768450814079, 'roc_auc_score': 0.6996445686900958}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
97,187.49308057129383,0:00:02,23.66234764456749,0:00:00,"{'f1': 0.5292269013199246, 'precision': 0.7738970588235294, 'accuracy': 0.8296178343949044, 'roc_auc_score': 0.6826869301674678}",54.238351583480835,"{'f1': 0.5570558644426794, 'precision': 0.7821561338289963, 'accuracy': 0.8368918787169738, 'roc_auc_score': 0.6975608500084076}",0:00:00,20220117_1724,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
98,189.2313652932644,0:00:02,23.67859961092472,0:00:00,"{'f1': 0.546353522867738, 'precision': 0.7740805604203153, 'accuracy': 0.8330300272975433, 'roc_auc_score': 0.6918197936746004}",54.09845332801342,"{'f1': 0.5691987513007284, 'precision': 0.7747875354107648, 'accuracy': 0.8385492834162036, 'roc_auc_score': 0.7045982743400033}",0:00:00,20220117_1725,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
99,187.5394981354475,0:00:02,23.676682770252228,0:00:00,"{'f1': 0.5349127182044887, 'precision': 0.770197486535009, 'accuracy': 0.8303002729754322, 'roc_auc_score': 0.6857608780279962}",54.24778279662132,"{'f1': 0.5616618459111228, 'precision': 0.7789934354485777, 'accuracy': 0.8374768450814079, 'roc_auc_score': 0.7002113460568354}",0:00:00,20220117_1725,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss
100,187.82038192451,0:00:02,23.61914075911045,0:00:00,"{'f1': 0.5522296884544898, 'precision': 0.7661016949152543, 'accuracy': 0.8332575068243858, 'roc_auc_score': 0.6952516581807624}",54.24645994603634,"{'f1': 0.5756951596292481, 'precision': 0.7699724517906336, 'accuracy': 0.8393292385687823, 'roc_auc_score': 0.7085101206490667}",0:00:00,20220117_1725,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_deepcross/results/Model_LR0.0005_Batch64_LossBCEWithLogitsLoss

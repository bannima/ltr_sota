Epoch,Train Loss,Trian Time,Valid Loss,Valid Time,Eval Metrics,Test Loss,Test Metrics,Test Time,Time,Test Result Dir
1,113.36455745995045,0:00:24,12.839677691459656,0:00:01,"{'f1': 0.5170913610938471, 'precision': 0.7402135231316725, 'accuracy': 0.8232484076433121, 'roc_auc_score': 0.6768652947194034}",29.752272725105286,"{'f1': 0.5270341207349082, 'precision': 0.7285921625544267, 'accuracy': 0.8243151018816418, 'roc_auc_score': 0.6825167101059357}",0:00:04,20220119_1558,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.002_Batch128_LossBCEWithLogitsLoss
2,105.17103530466557,0:00:24,12.710107177495956,0:00:01,"{'f1': 0.5458879618593564, 'precision': 0.7258320126782885, 'accuracy': 0.8266606005459509, 'roc_auc_score': 0.6928915472636772}",28.805552780628204,"{'f1': 0.568193256165073, 'precision': 0.732166018158236, 'accuracy': 0.8326996197718631, 'roc_auc_score': 0.7057237104842778}",0:00:04,20220119_1559,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.002_Batch128_LossBCEWithLogitsLoss
3,103.04385948181152,0:00:25,12.504057824611664,0:00:01,"{'f1': 0.5149625935162094, 'precision': 0.7414721723518851, 'accuracy': 0.8230209281164695, 'roc_auc_score': 0.6757312265589551}",28.509574875235558,"{'f1': 0.5378947368421053, 'precision': 0.7470760233918129, 'accuracy': 0.8287998440089694, 'roc_auc_score': 0.6880065053808643}",0:00:04,20220119_1559,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.002_Batch128_LossBCEWithLogitsLoss
4,101.41005665063858,0:00:25,12.370274618268013,0:00:01,"{'f1': 0.5128526645768025, 'precision': 0.7463503649635036, 'accuracy': 0.8232484076433121, 'roc_auc_score': 0.6745674983736896}",28.56494465470314,"{'f1': 0.538239159001314, 'precision': 0.7458120903131829, 'accuracy': 0.8287023496148972, 'roc_auc_score': 0.6882259963006557}",0:00:04,20220119_1600,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.002_Batch128_LossBCEWithLogitsLoss
5,100.5503011494875,0:00:25,12.415907353162766,0:00:01,"{'f1': 0.5109855618330195, 'precision': 0.7454212454212454, 'accuracy': 0.8227934485896269, 'roc_auc_score': 0.6736123885360581}",28.221732139587402,"{'f1': 0.5315243415802074, 'precision': 0.7528259231348907, 'accuracy': 0.8283123720386077, 'roc_auc_score': 0.6844280467042206}",0:00:04,20220119_1600,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.002_Batch128_LossBCEWithLogitsLoss
6,99.30901016294956,0:00:25,12.48725751042366,0:00:01,"{'f1': 0.4945686900958466, 'precision': 0.747104247104247, 'accuracy': 0.8200636942675159, 'roc_auc_score': 0.6652556765437401}",28.483209863305092,"{'f1': 0.5296632816675575, 'precision': 0.7564885496183206, 'accuracy': 0.8284098664326801, 'roc_auc_score': 0.6833583897343198}",0:00:04,20220119_1601,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.002_Batch128_LossBCEWithLogitsLoss
7,99.03477343916893,0:00:24,12.368269383907318,0:00:01,"{'f1': 0.5336597307221543, 'precision': 0.7427597955706985, 'accuracy': 0.8266606005459509, 'roc_auc_score': 0.6856699016057196}",28.61575835943222,"{'f1': 0.5511363636363636, 'precision': 0.7409722222222223, 'accuracy': 0.8305547431022716, 'roc_auc_score': 0.695532910501093}",0:00:04,20220119_1601,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.002_Batch128_LossBCEWithLogitsLoss
8,97.72281990945339,0:00:24,12.373154729604721,0:00:01,"{'f1': 0.5019059720457433, 'precision': 0.7495256166982922, 'accuracy': 0.821656050955414, 'roc_auc_score': 0.668926817596266}",28.33884283900261,"{'f1': 0.5240641711229947, 'precision': 0.7492354740061162, 'accuracy': 0.8264599785512333, 'roc_auc_score': 0.6805217967042206}",0:00:04,20220119_1602,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.002_Batch128_LossBCEWithLogitsLoss
9,97.80453830957413,0:00:24,12.428391993045807,0:00:01,"{'f1': 0.4938906752411576, 'precision': 0.7559055118110236, 'accuracy': 0.8209736123748863, 'roc_auc_score': 0.6648680998732889}",28.19647392630577,"{'f1': 0.521176153223631, 'precision': 0.7576470588235295, 'accuracy': 0.826947450521595, 'roc_auc_score': 0.6788575647385237}",0:00:04,20220119_1602,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.002_Batch128_LossBCEWithLogitsLoss
10,97.11131837964058,0:00:24,12.31215289235115,0:00:01,"{'f1': 0.5025380710659898, 'precision': 0.7485822306238186, 'accuracy': 0.821656050955414, 'roc_auc_score': 0.6692550742170823}",28.44560667872429,"{'f1': 0.5292077887436649, 'precision': 0.7532270311313591, 'accuracy': 0.8279223944623184, 'roc_auc_score': 0.6831805952581134}",0:00:04,20220119_1603,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.002_Batch128_LossBCEWithLogitsLoss

Epoch,Train Loss,Trian Time,Valid Loss,Valid Time,Eval Metrics,Test Loss,Test Metrics,Test Time,Time,Test Result Dir
1,244.8872835636139,0:00:05,26.277996256947517,0:00:00,"{'f1': 0.37509051412020267, 'precision': 0.7754491017964071, 'accuracy': 0.8036851683348498, 'roc_auc_score': 0.6124893516233016}",60.33539493381977,"{'f1': 0.3778120184899846, 'precision': 0.7539975399753998, 'accuracy': 0.8031588183679439, 'roc_auc_score': 0.6132484078106608}",0:00:01,20220119_1547,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.005_Batch64_LossBCEWithLogitsLoss
2,216.0387108027935,0:00:05,24.666601464152336,0:00:00,"{'f1': 0.5226006191950464, 'precision': 0.7429577464788732, 'accuracy': 0.8246132848043676, 'roc_auc_score': 0.6797306242322974}",57.06041461229324,"{'f1': 0.5374077976817703, 'precision': 0.7478005865102639, 'accuracy': 0.8287998440089694, 'roc_auc_score': 0.6877231166974944}",0:00:01,20220119_1547,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.005_Batch64_LossBCEWithLogitsLoss
3,205.6600226163864,0:00:04,24.646018907427788,0:00:00,"{'f1': 0.501269035532995, 'precision': 0.7466918714555766, 'accuracy': 0.8212010919017289, 'roc_auc_score': 0.6686282210002672}",57.1460542678833,"{'f1': 0.5263439422305429, 'precision': 0.7528691660290742, 'accuracy': 0.8273374280978844, 'roc_auc_score': 0.6816636539431645}",0:00:01,20220119_1547,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.005_Batch64_LossBCEWithLogitsLoss
4,201.88941073417664,0:00:04,24.58449400961399,0:00:00,"{'f1': 0.4913848117421825, 'precision': 0.7403846153846154, 'accuracy': 0.8186988171064604, 'roc_auc_score': 0.6637033735141112}",57.35700239241123,"{'f1': 0.5170104473613715, 'precision': 0.7417371252882398, 'accuracy': 0.8242176074875694, 'roc_auc_score': 0.676926733016647}",0:00:01,20220119_1547,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.005_Batch64_LossBCEWithLogitsLoss
5,196.75847733020782,0:00:04,24.252741441130638,0:00:00,"{'f1': 0.45503355704697995, 'precision': 0.7652370203160271, 'accuracy': 0.8152866242038217, 'roc_auc_score': 0.6463640944865723}",56.19460469484329,"{'f1': 0.4933035714285714, 'precision': 0.7673611111111112, 'accuracy': 0.8229501803646291, 'roc_auc_score': 0.664618820413654}",0:00:01,20220119_1547,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.005_Batch64_LossBCEWithLogitsLoss
6,196.27694818377495,0:00:04,24.224965780973434,0:00:00,"{'f1': 0.4821775761503565, 'precision': 0.75, 'accuracy': 0.8182438580527752, 'roc_auc_score': 0.659137440847501}",56.19125625491142,"{'f1': 0.523060227889311, 'precision': 0.7687400318979266, 'accuracy': 0.8286048552208248, 'roc_auc_score': 0.6796604380359845}",0:00:01,20220119_1547,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.005_Batch64_LossBCEWithLogitsLoss
7,193.15698119997978,0:00:04,24.268099308013916,0:00:00,"{'f1': 0.48235294117647065, 'precision': 0.7639751552795031, 'accuracy': 0.8198362147406734, 'roc_auc_score': 0.659197759071048}",55.98429794609547,"{'f1': 0.5168478260869566, 'precision': 0.7620192307692307, 'accuracy': 0.8266549673393779, 'roc_auc_score': 0.6765404563225156}",0:00:01,20220119_1547,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.005_Batch64_LossBCEWithLogitsLoss
8,193.14696553349495,0:00:04,24.181955575942993,0:00:00,"{'f1': 0.49034749034749037, 'precision': 0.7514792899408284, 'accuracy': 0.8198362147406734, 'roc_auc_score': 0.6631368385208432}",56.06151093542576,"{'f1': 0.5279870828848223, 'precision': 0.764018691588785, 'accuracy': 0.8289948327971142, 'roc_auc_score': 0.6823248328989405}",0:00:01,20220119_1547,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.005_Batch64_LossBCEWithLogitsLoss
9,190.53423915803432,0:00:05,24.131229668855667,0:00:00,"{'f1': 0.4864516129032259, 'precision': 0.7495029821073559, 'accuracy': 0.818926296633303, 'roc_auc_score': 0.6612266188455805}",55.667659386992455,"{'f1': 0.5317204301075269, 'precision': 0.7678571428571429, 'accuracy': 0.8301647655259823, 'roc_auc_score': 0.684225160795359}",0:00:01,20220119_1547,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.005_Batch64_LossBCEWithLogitsLoss
10,190.48074179887772,0:00:05,24.148695200681686,0:00:00,"{'f1': 0.493573264781491, 'precision': 0.7544204322200393, 'accuracy': 0.8207461328480437, 'roc_auc_score': 0.6647188015752895}",55.50392599403858,"{'f1': 0.5314009661835748, 'precision': 0.7650695517774343, 'accuracy': 0.8297747879496928, 'roc_auc_score': 0.6841112640827308}",0:00:01,20220119_1547,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_fm/results/Model_LR0.005_Batch64_LossBCEWithLogitsLoss

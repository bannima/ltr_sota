Epoch,Train Loss,Trian Time,Valid Loss,Valid Time,Eval Metrics,Test Loss,Test Metrics,Test Time,Time,Test Result Dir
1,110.42769938707352,0:00:05,14.799133360385895,0:00:00,"{'f1': 0.13519091847265222, 'precision': 0.14702581369248036, 'accuracy': 0.618744313011829, 'roc_auc_score': 0.4490929878853058}",33.86254274845123,"{'f1': 0.13864894795127353, 'precision': 0.1502640422467595, 'accuracy': 0.6208443014526664, 'roc_auc_score': 0.45125128741382203}",0:00:01,20211230_1733,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
2,108.69268500804901,0:00:05,14.18457144498825,0:00:00,"{'f1': 0.1727408886669995, 'precision': 0.1809623430962343, 'accuracy': 0.623066424021838, 'roc_auc_score': 0.46571643362157744}",32.39220970869064,"{'f1': 0.17539378458918692, 'precision': 0.18181818181818182, 'accuracy': 0.6223067173637515, 'roc_auc_score': 0.4662374936942996}",0:00:01,20211230_1733,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
3,104.9528813958168,0:00:05,13.765892446041107,0:00:00,"{'f1': 0.22394571013087736, 'precision': 0.22736220472440946, 'accuracy': 0.6358052775250227, 'roc_auc_score': 0.4931160223168871}",30.98821246623993,"{'f1': 0.2333127952351612, 'precision': 0.23307345096430038, 'accuracy': 0.6360534269279516, 'roc_auc_score': 0.4973513956616781}",0:00:01,20211230_1733,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
4,103.06531357765198,0:00:05,13.397433519363403,0:00:00,"{'f1': 0.25357483317445184, 'precision': 0.2530922930542341, 'accuracy': 0.6437670609645132, 'roc_auc_score': 0.5098304444754354}",30.00192403793335,"{'f1': 0.27154046997389036, 'precision': 0.2654102866117, 'accuracy': 0.6463878326996197, 'roc_auc_score': 0.5194275475029426}",0:00:01,20211230_1734,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
5,102.4374732375145,0:00:05,13.195235431194305,0:00:00,"{'f1': 0.2514563106796117, 'precision': 0.25567620927936824, 'accuracy': 0.6492265696087353, 'roc_auc_score': 0.5111158072817071}",29.714676678180695,"{'f1': 0.27021494370522003, 'precision': 0.26905829596412556, 'accuracy': 0.652432485132105, 'roc_auc_score': 0.5211220993778376}",0:00:01,20211230_1734,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
6,101.5726163983345,0:00:05,13.100009739398956,0:00:00,"{'f1': 0.27793696275071633, 'precision': 0.27793696275071633, 'accuracy': 0.6560509554140127, 'roc_auc_score': 0.5260989680878096}",29.424704909324646,"{'f1': 0.30288364589635003, 'precision': 0.297190344281757, 'accuracy': 0.6629618796919177, 'roc_auc_score': 0.5409172429376157}",0:00:01,20211230_1734,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
7,100.7929036617279,0:00:05,12.9900461435318,0:00:00,"{'f1': 0.29668246445497637, 'precision': 0.2944496707431797, 'accuracy': 0.6624203821656051, 'roc_auc_score': 0.5375009660897507}",29.429910957813263,"{'f1': 0.3133880328196918, 'precision': 0.30526315789473685, 'accuracy': 0.6654967339377986, 'roc_auc_score': 0.547112803724567}",0:00:01,20211230_1734,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
8,100.10697841644287,0:00:05,12.887179553508759,0:00:00,"{'f1': 0.2994241842610365, 'precision': 0.3008678881388621, 'accuracy': 0.6678798908098271, 'roc_auc_score': 0.5407558686209202}",29.37559872865677,"{'f1': 0.3151515151515152, 'precision': 0.30976965845909454, 'accuracy': 0.6694940040947646, 'roc_auc_score': 0.5493075290062216}",0:00:01,20211230_1734,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
9,99.51042687892914,0:00:06,12.866278052330017,0:00:00,"{'f1': 0.2859931673987311, 'precision': 0.29241516966067865, 'accuracy': 0.6671974522292994, 'roc_auc_score': 0.5340710979314128}",28.676919519901276,"{'f1': 0.3059257740414189, 'precision': 0.3051124744376278, 'accuracy': 0.6699814760651263, 'roc_auc_score': 0.544809410206827}",0:00:01,20211230_1734,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
10,98.79483956098557,0:00:05,12.76840353012085,0:00:00,"{'f1': 0.283743842364532, 'precision': 0.29298067141403866, 'accuracy': 0.6692447679708826, 'roc_auc_score': 0.5337734995093262}",28.612285614013672,"{'f1': 0.3058823529411765, 'precision': 0.30708661417322836, 'accuracy': 0.6721263527347178, 'roc_auc_score': 0.5455066892971246}",0:00:01,20211230_1734,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
11,98.18946474790573,0:00:05,12.630496799945831,0:00:00,"{'f1': 0.2837162837162837, 'precision': 0.2973821989528796, 'accuracy': 0.6737943585077343, 'roc_auc_score': 0.5354464389860493}",28.351742148399353,"{'f1': 0.3019419503027772, 'precision': 0.30674586338565973, 'accuracy': 0.6740762406161646, 'roc_auc_score': 0.5442341464183622}",0:00:01,20211230_1734,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
12,97.65795356035233,0:00:05,12.541593074798584,0:00:00,"{'f1': 0.3016264169541646, 'precision': 0.31160896130346233, 'accuracy': 0.6776615104640582, 'roc_auc_score': 0.5452061557099969}",27.99768453836441,"{'f1': 0.3179190751445087, 'precision': 0.31923714759535654, 'accuracy': 0.6778785219849859, 'roc_auc_score': 0.5533857932571045}",0:00:01,20211230_1734,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
13,97.57665407657623,0:00:05,12.390771448612213,0:00:00,"{'f1': 0.285570638511815, 'precision': 0.30148619957537154, 'accuracy': 0.6767515923566879, 'roc_auc_score': 0.5373873168600416}",28.008323073387146,"{'f1': 0.30905636478784043, 'precision': 0.3175704989154013, 'accuracy': 0.6809008482012284, 'roc_auc_score': 0.5499822389440054}",0:00:01,20211230_1734,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
14,96.64011830091476,0:00:05,12.44777512550354,0:00:00,"{'f1': 0.2900763358778626, 'precision': 0.3104575163398693, 'accuracy': 0.6826660600545951, 'roc_auc_score': 0.5415973292288423}",27.84469836950302,"{'f1': 0.3056861906797776, 'precision': 0.31834372217275153, 'accuracy': 0.6833382080530369, 'roc_auc_score': 0.5491708792248192}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
15,96.23962044715881,0:00:05,12.407370269298553,0:00:00,"{'f1': 0.3119083208769307, 'precision': 0.3260416666666667, 'accuracy': 0.6858507734303912, 'roc_auc_score': 0.5528786907836891}",27.72977203130722,"{'f1': 0.3263757115749526, 'precision': 0.33491994807442665, 'accuracy': 0.688505410938871, 'roc_auc_score': 0.5609174268538759}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
16,95.63610297441483,0:00:05,12.26727420091629,0:00:00,"{'f1': 0.3095477386934673, 'precision': 0.3266171792152704, 'accuracy': 0.6874431301182894, 'roc_auc_score': 0.5522824957656036}",27.28663659095764,"{'f1': 0.3262441514249256, 'precision': 0.3378854625550661, 'accuracy': 0.6911377595788242, 'roc_auc_score': 0.5616508060786951}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
17,95.34388154745102,0:00:05,12.126663208007812,0:00:00,"{'f1': 0.3018292682926829, 'precision': 0.32247557003257327, 'accuracy': 0.6874431301182894, 'roc_auc_score': 0.5486716729366249}",27.553560316562653,"{'f1': 0.3175355450236967, 'precision': 0.3334841628959276, 'accuracy': 0.6911377595788242, 'roc_auc_score': 0.5573999758281486}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
18,94.35456305742264,0:00:05,11.933144867420197,0:00:00,"{'f1': 0.3174284279256655, 'precision': 0.3347457627118644, 'accuracy': 0.6908553230209281, 'roc_auc_score': 0.5571480232021248}",27.13303232192993,"{'f1': 0.33447098976109213, 'precision': 0.3475177304964539, 'accuracy': 0.6958174904942965, 'roc_auc_score': 0.5671267025390954}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
19,93.6365876197815,0:00:05,12.138911187648773,0:00:00,"{'f1': 0.3086356668369954, 'precision': 0.33186813186813185, 'accuracy': 0.6922202001819836, 'roc_auc_score': 0.5534482202986936}",27.09289437532425,"{'f1': 0.32957990472065835, 'precision': 0.348124428179323, 'accuracy': 0.6981573559520328, 'roc_auc_score': 0.5654012790062216}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
20,93.8429611325264,0:00:05,11.93159943819046,0:00:00,"{'f1': 0.2998965873836608, 'precision': 0.3269447576099211, 'accuracy': 0.6919927206551411, 'roc_auc_score': 0.549359842550899}",26.827060222625732,"{'f1': 0.3217028746982664, 'precision': 0.34494117647058825, 'accuracy': 0.6986448279223945, 'roc_auc_score': 0.5617533262569362}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
21,93.19169908761978,0:00:05,11.978460669517517,0:00:00,"{'f1': 0.29864724245577523, 'precision': 0.328, 'accuracy': 0.6933575978161965, 'roc_auc_score': 0.5492708624764466}",26.904666543006897,"{'f1': 0.3204957945993803, 'precision': 0.34707574304889743, 'accuracy': 0.7006922101979136, 'roc_auc_score': 0.5618199302169161}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
22,92.78711199760437,0:00:05,11.817375600337982,0:00:00,"{'f1': 0.3045054375970999, 'precision': 0.332579185520362, 'accuracy': 0.6944949954504095, 'roc_auc_score': 0.5523151503121575}",26.60122239589691,"{'f1': 0.3263598326359833, 'precision': 0.35135135135135137, 'accuracy': 0.7017646485327094, 'roc_auc_score': 0.5649316094249202}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
23,92.37013447284698,0:00:05,11.640756607055664,0:00:00,"{'f1': 0.29931972789115646, 'precision': 0.33101851851851855, 'accuracy': 0.6954049135577798, 'roc_auc_score': 0.5502862905376251}",26.664182007312775,"{'f1': 0.31935771632471005, 'precision': 0.3489278752436647, 'accuracy': 0.7024471092912158, 'roc_auc_score': 0.561836535227846}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
24,91.80578345060349,0:00:05,11.554745018482208,0:00:00,"{'f1': 0.30073606729758146, 'precision': 0.3345029239766082, 'accuracy': 0.697452229299363, 'roc_auc_score': 0.5516299752196197}",26.244432985782623,"{'f1': 0.32312849162011176, 'precision': 0.35389133627019087, 'accuracy': 0.7046894803548795, 'roc_auc_score': 0.5642980441819405}",0:00:01,20211230_1735,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
25,91.53379142284393,0:00:05,11.606065690517426,0:00:00,"{'f1': 0.29421136484333515, 'precision': 0.33133971291866027, 'accuracy': 0.6976797088262057, 'roc_auc_score': 0.5488249639302728}",26.335476458072662,"{'f1': 0.31828442437923254, 'precision': 0.35285285285285284, 'accuracy': 0.7055669299015307, 'roc_auc_score': 0.562322625903817}",0:00:01,20211230_1736,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
26,91.01229971647263,0:00:05,11.520060539245605,0:00:00,"{'f1': 0.30996309963099633, 'precision': 0.3458823529411765, 'accuracy': 0.7022292993630573, 'roc_auc_score': 0.5573912924441371}",25.92286628484726,"{'f1': 0.33132932531730125, 'precision': 0.36134045653229724, 'accuracy': 0.7072243346007605, 'roc_auc_score': 0.5689349672103582}",0:00:01,20211230_1736,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
27,90.91819453239441,0:00:05,11.475171267986298,0:00:00,"{'f1': 0.30613333333333337, 'precision': 0.3466183574879227, 'accuracy': 0.704049135577798, 'roc_auc_score': 0.5562878824824186}",25.77367788553238,"{'f1': 0.3270270270270271, 'precision': 0.3615537848605578, 'accuracy': 0.7086867505118456, 'roc_auc_score': 0.5673429355137044}",0:00:01,20211230_1736,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
28,90.14104872941971,0:00:05,11.49937790632248,0:00:00,"{'f1': 0.29673913043478256, 'precision': 0.3442622950819672, 'accuracy': 0.7056414922656961, 'roc_auc_score': 0.5527373778769867}",25.934479355812073,"{'f1': 0.32104419509961074, 'precision': 0.36227390180878555, 'accuracy': 0.7109291215755094, 'roc_auc_score': 0.5652702255338826}",0:00:01,20211230_1736,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
29,90.04531806707382,0:00:05,11.357492625713348,0:00:00,"{'f1': 0.2908891328210757, 'precision': 0.3419354838709677, 'accuracy': 0.7060964513193813, 'roc_auc_score': 0.5504099215064554}",25.680993914604187,"{'f1': 0.3143318716824371, 'precision': 0.3582325092056812, 'accuracy': 0.7103441552110754, 'roc_auc_score': 0.5620529521187153}",0:00:01,20211230_1736,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
30,89.14877444505692,0:00:05,11.361537754535675,0:00:00,"{'f1': 0.29262926292629265, 'precision': 0.3450064850843061, 'accuracy': 0.7074613284804367, 'roc_auc_score': 0.5516339679152682}",25.626109898090363,"{'f1': 0.31795108444854636, 'precision': 0.36225026288117773, 'accuracy': 0.7118065711221605, 'roc_auc_score': 0.5641449733058685}",0:00:01,20211230_1736,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
31,88.9415317773819,0:00:05,11.368814289569855,0:00:00,"{'f1': 0.306941431670282, 'precision': 0.35508155583437895, 'accuracy': 0.7092811646951774, 'roc_auc_score': 0.5584087168531399}",25.215156763792038,"{'f1': 0.32943871706758304, 'precision': 0.371960682876358, 'accuracy': 0.7146339085502583, 'roc_auc_score': 0.570248838700185}",0:00:01,20211230_1736,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
32,88.87718933820724,0:00:05,11.197621405124664,0:00:00,"{'f1': 0.29392265193370165, 'precision': 0.3486238532110092, 'accuracy': 0.7092811646951774, 'roc_auc_score': 0.5528283542992634}",25.391380488872528,"{'f1': 0.31743076565045375, 'precision': 0.3656836461126005, 'accuracy': 0.7140489421858243, 'roc_auc_score': 0.5646227614763747}",0:00:01,20211230_1736,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
33,88.67322033643723,0:00:05,11.21422016620636,0:00:00,"{'f1': 0.2869855394883204, 'precision': 0.34354194407456723, 'accuracy': 0.7083712465878071, 'roc_auc_score': 0.5496051081407357}",25.26881033182144,"{'f1': 0.31267605633802814, 'precision': 0.36433260393873085, 'accuracy': 0.7145364141561861, 'roc_auc_score': 0.562675140827308}",0:00:01,20211230_1736,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
34,88.28268295526505,0:00:05,11.17789489030838,0:00:00,"{'f1': 0.2866741321388578, 'precision': 0.34641407307171856, 'accuracy': 0.7101910828025477, 'roc_auc_score': 0.5501429812830985}",25.300988137722015,"{'f1': 0.3116454502593117, 'precision': 0.3651933701657459, 'accuracy': 0.7153163693087647, 'roc_auc_score': 0.5624778512275097}",0:00:01,20211230_1736,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
35,87.76256918907166,0:00:05,11.118507385253906,0:00:00,"{'f1': 0.2997787610619469, 'precision': 0.35611038107752957, 'accuracy': 0.7120109190172884, 'roc_auc_score': 0.5562612169793375}",24.900527149438858,"{'f1': 0.326198231735691, 'precision': 0.37566988210075025, 'accuracy': 0.717656234766501, 'roc_auc_score': 0.5696791712207836}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
36,87.41361647844315,0:00:05,11.213932275772095,0:00:00,"{'f1': 0.2991071428571429, 'precision': 0.3597315436241611, 'accuracy': 0.7142857142857143, 'roc_auc_score': 0.5567694300968827}",25.045390903949738,"{'f1': 0.32549388523047973, 'precision': 0.3802197802197802, 'accuracy': 0.7203860778005264, 'roc_auc_score': 0.5701930595258113}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
37,87.30436599254608,0:00:05,10.992326885461807,0:00:00,"{'f1': 0.29471316085489313, 'precision': 0.35841313269493846, 'accuracy': 0.7147406733393995, 'roc_auc_score': 0.5550984869679839}",24.713574171066284,"{'f1': 0.3236475313016773, 'precision': 0.38034425319267073, 'accuracy': 0.7208735497708881, 'roc_auc_score': 0.5695206879519085}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
38,86.91302824020386,0:00:05,10.978896796703339,0:00:00,"{'f1': 0.29642248722316866, 'precision': 0.36554621848739494, 'accuracy': 0.7181528662420382, 'roc_auc_score': 0.5570097048171587}",24.957309901714325,"{'f1': 0.32526115859449195, 'precision': 0.3848314606741573, 'accuracy': 0.7229209320464073, 'roc_auc_score': 0.5708625409870522}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
39,86.58977746963501,0:00:05,11.039885640144348,0:00:00,"{'f1': 0.2885057471264368, 'precision': 0.3621933621933622, 'accuracy': 0.7183803457688808, 'roc_auc_score': 0.5538764369069956}",24.456793934106827,"{'f1': 0.31869450443964475, 'precision': 0.38270893371757925, 'accuracy': 0.7232134152286244, 'roc_auc_score': 0.5680786531024046}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
40,86.62043327093124,0:00:05,10.97155487537384,0:00:00,"{'f1': 0.2908045977011494, 'precision': 0.36507936507936506, 'accuracy': 0.7192902638762512, 'roc_auc_score': 0.5551301433406257}",24.582433342933655,"{'f1': 0.3221637146960268, 'precision': 0.3854524627720504, 'accuracy': 0.7238958759871308, 'roc_auc_score': 0.5698011865226165}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
41,86.1121786236763,0:00:05,10.914585769176483,0:00:00,"{'f1': 0.29116117850953205, 'precision': 0.3684210526315789, 'accuracy': 0.7208826205641492, 'roc_auc_score': 0.5558469748058052}",24.71861243247986,"{'f1': 0.3201542912246866, 'precision': 0.38694638694638694, 'accuracy': 0.7250658087159988, 'roc_auc_score': 0.5692927106103918}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
42,86.11859321594238,0:00:05,10.958701729774475,0:00:00,"{'f1': 0.28471001757469244, 'precision': 0.36818181818181817, 'accuracy': 0.7222474977252047, 'roc_auc_score': 0.5537884550064553}",24.45724231004715,"{'f1': 0.3130012150668286, 'precision': 0.38265002970885326, 'accuracy': 0.7243833479574925, 'roc_auc_score': 0.5660115394316462}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
43,85.63614803552628,0:00:05,10.690422803163528,0:00:00,"{'f1': 0.2966396292004635, 'precision': 0.37702503681885124, 'accuracy': 0.7238398544131028, 'roc_auc_score': 0.5591008791630626}",24.347232460975647,"{'f1': 0.3216783216783217, 'precision': 0.3889212827988338, 'accuracy': 0.7257482694745052, 'roc_auc_score': 0.5701650779804943}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
44,85.5420835018158,0:00:05,10.796466171741486,0:00:00,"{'f1': 0.2822014051522248, 'precision': 0.36459909228441756, 'accuracy': 0.7211101000909919, 'roc_auc_score': 0.5523854502748258}",24.212850868701935,"{'f1': 0.3154313487241798, 'precision': 0.38562091503267976, 'accuracy': 0.7253582918982159, 'roc_auc_score': 0.5673589887758534}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
45,85.18794167041779,0:00:05,10.81083059310913,0:00:00,"{'f1': 0.3001158748551564, 'precision': 0.38144329896907214, 'accuracy': 0.7252047315741583, 'roc_auc_score': 0.5609814388135077}",23.924600183963776,"{'f1': 0.3277027027027027, 'precision': 0.3966121495327103, 'accuracy': 0.7283806181144584, 'roc_auc_score': 0.5735906496973263}",0:00:01,20211230_1737,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
46,84.45117908716202,0:00:05,10.633210748434067,0:00:00,"{'f1': 0.29836829836829837, 'precision': 0.38266068759342303, 'accuracy': 0.7261146496815286, 'roc_auc_score': 0.5605938621430566}",24.209154427051544,"{'f1': 0.3265503875968992, 'precision': 0.39740566037735847, 'accuracy': 0.7289655844788925, 'roc_auc_score': 0.5732655645703717}",0:00:01,20211230_1738,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
47,84.64419507980347,0:00:05,10.724318504333496,0:00:00,"{'f1': 0.29807130333138515, 'precision': 0.38403614457831325, 'accuracy': 0.7267970882620565, 'roc_auc_score': 0.5607135004162386}",23.92194002866745,"{'f1': 0.3266197524872604, 'precision': 0.3984606275902901, 'accuracy': 0.7294530564492542, 'roc_auc_score': 0.5734433590465782}",0:00:01,20211230_1738,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
48,84.4385911822319,0:00:05,10.79142040014267,0:00:00,"{'f1': 0.2841918294849023, 'precision': 0.37383177570093457, 'accuracy': 0.7249772520473158, 'roc_auc_score': 0.5545952647199994}",23.85919350385666,"{'f1': 0.31878371750858264, 'precision': 0.39489671931956255, 'accuracy': 0.7291605732670371, 'roc_auc_score': 0.569992695897091}",0:00:01,20211230_1738,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
49,84.03781712055206,0:00:05,10.564368546009064,0:00:00,"{'f1': 0.30005837711617045, 'precision': 0.3858858858858859, 'accuracy': 0.7272520473157416, 'roc_auc_score': 0.5616686102538699}",23.821879267692566,"{'f1': 0.32854009261515965, 'precision': 0.40335128665469777, 'accuracy': 0.731402944330701, 'roc_auc_score': 0.5748630086598284}",0:00:01,20211230_1738,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
50,83.87967312335968,0:00:05,10.59949392080307,0:00:00,"{'f1': 0.30040911747516075, 'precision': 0.3870481927710843, 'accuracy': 0.7277070063694268, 'roc_auc_score': 0.5619672068498687}",23.6925508081913,"{'f1': 0.33309020179917337, 'precision': 0.40749553837001784, 'accuracy': 0.7325728770595691, 'roc_auc_score': 0.5771884195813015}",0:00:01,20211230_1738,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
51,83.67157429456711,0:00:05,10.501519858837128,0:00:00,"{'f1': 0.3049853372434017, 'precision': 0.3951367781155015, 'accuracy': 0.7304367606915377, 'roc_auc_score': 0.5647435562883103}",23.597181916236877,"{'f1': 0.3364303178484108, 'precision': 0.4149577804583836, 'accuracy': 0.735400214487667, 'roc_auc_score': 0.5794665377501261}",0:00:01,20211230_1738,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
52,83.09215712547302,0:00:05,10.55109429359436,0:00:00,"{'f1': 0.2776101388050694, 'precision': 0.3770491803278688, 'accuracy': 0.7277070063694268, 'roc_auc_score': 0.5531042780878297}",23.5799360871315,"{'f1': 0.3133917396745933, 'precision': 0.40051183621241204, 'accuracy': 0.7325728770595691, 'roc_auc_score': 0.5688284534218934}",0:00:01,20211230_1738,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
53,83.33003574609756,0:00:05,10.561044752597809,0:00:00,"{'f1': 0.30206489675516224, 'precision': 0.3950617283950617, 'accuracy': 0.7308917197452229, 'roc_auc_score': 0.5637291264010441}",23.220936805009842,"{'f1': 0.337573385518591, 'precision': 0.4166666666666667, 'accuracy': 0.735985180852101, 'roc_auc_score': 0.5801333130149655}",0:00:01,20211230_1738,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
54,83.23213350772858,0:00:05,10.45408546924591,0:00:00,"{'f1': 0.28571428571428575, 'precision': 0.3872549019607843, 'accuracy': 0.7304367606915377, 'roc_auc_score': 0.5571936540095362}",23.32503256201744,"{'f1': 0.31884057971014496, 'precision': 0.4063694267515924, 'accuracy': 0.7342302817587989, 'roc_auc_score': 0.5716150475029427}",0:00:01,20211230_1738,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
55,82.93541461229324,0:00:05,10.333121925592422,0:00:00,"{'f1': 0.28948948948948944, 'precision': 0.38996763754045305, 'accuracy': 0.7308917197452229, 'roc_auc_score': 0.5588052770888}",23.52209359407425,"{'f1': 0.32554671968190857, 'precision': 0.4114321608040201, 'accuracy': 0.735400214487667, 'roc_auc_score': 0.5747906244745249}",0:00:01,20211230_1738,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
56,82.63779038190842,0:00:05,10.540543973445892,0:00:00,"{'f1': 0.28381642512077293, 'precision': 0.38587848932676516, 'accuracy': 0.7302092811646952, 'roc_auc_score': 0.5563878424699044}",23.485549867153168,"{'f1': 0.32191952011997, 'precision': 0.41045251752708733, 'accuracy': 0.7354977088817393, 'roc_auc_score': 0.5732958844795696}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
57,82.36005330085754,0:00:05,10.376783013343811,0:00:00,"{'f1': 0.29001203369434414, 'precision': 0.39186991869918697, 'accuracy': 0.7315741583257507, 'roc_auc_score': 0.5592531719827982}",23.461756706237793,"{'f1': 0.3266118994274334, 'precision': 0.4138801261829653, 'accuracy': 0.736277664034318, 'roc_auc_score': 0.5755073986884144}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
58,82.52163815498352,0:00:05,10.375427961349487,0:00:00,"{'f1': 0.28692493946731235, 'precision': 0.39173553719008264, 'accuracy': 0.7320291173794359, 'roc_auc_score': 0.5582387420955321}",23.265599489212036,"{'f1': 0.32441889527618095, 'precision': 0.41363926067558954, 'accuracy': 0.7364726528224627, 'roc_auc_score': 0.5746433338237767}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
59,82.06469684839249,0:00:05,10.443189263343811,0:00:00,"{'f1': 0.2965186074429772, 'precision': 0.3990306946688207, 'accuracy': 0.7333939945404914, 'roc_auc_score': 0.562417098091691}",23.10457918047905,"{'f1': 0.3298507462686567, 'precision': 0.41750629722921917, 'accuracy': 0.7373501023691138, 'roc_auc_score': 0.5772021344795696}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
60,81.71552073955536,0:00:05,10.241577684879303,0:00:00,"{'f1': 0.2980769230769231, 'precision': 0.4019448946515397, 'accuracy': 0.7343039126478617, 'roc_auc_score': 0.563342547904505}",23.088492512702942,"{'f1': 0.3305908750934929, 'precision': 0.41988600379987334, 'accuracy': 0.7382275519157648, 'roc_auc_score': 0.577777214351774}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
61,81.69771075248718,0:00:05,10.328267335891724,0:00:00,"{'f1': 0.29812914906457455, 'precision': 0.40491803278688526, 'accuracy': 0.7354413102820746, 'roc_auc_score': 0.5637607827736857}",23.071492433547974,"{'f1': 0.32814851981936777, 'precision': 0.42084942084942084, 'accuracy': 0.7389100126742713, 'roc_auc_score': 0.5769492496216581}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
62,81.96742963790894,0:00:05,10.267275393009186,0:00:00,"{'f1': 0.2858895705521472, 'precision': 0.3996569468267582, 'accuracy': 0.7352138307552321, 'roc_auc_score': 0.5590158917842587}",23.307309567928314,"{'f1': 0.3182511438739197, 'precision': 0.4167776298268975, 'accuracy': 0.7385200350979819, 'roc_auc_score': 0.5727262170001681}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
63,81.37218794226646,0:00:05,10.361210703849792,0:00:00,"{'f1': 0.2937576499388005, 'precision': 0.4088586030664395, 'accuracy': 0.7374886260236578, 'roc_auc_score': 0.5628066711099665}",23.057298332452774,"{'f1': 0.32538071065989843, 'precision': 0.4250663129973475, 'accuracy': 0.740859900555718, 'roc_auc_score': 0.57638517845132}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
64,81.47887635231018,0:00:05,10.172848463058472,0:00:00,"{'f1': 0.2885326757090012, 'precision': 0.40695652173913044, 'accuracy': 0.7374886260236578, 'roc_auc_score': 0.560837131385069}",22.96869248151779,"{'f1': 0.32698654480832695, 'precision': 0.42733908427339085, 'accuracy': 0.7415423613142245, 'roc_auc_score': 0.5772575458214225}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
65,81.0078575015068,0:00:05,10.25817221403122,0:00:00,"{'f1': 0.298906439854192, 'precision': 0.41068447412353926, 'accuracy': 0.7374886260236578, 'roc_auc_score': 0.5647762108348641}",22.94868004322052,"{'f1': 0.33810600351670433, 'precision': 0.43447385409941897, 'accuracy': 0.7431022716193819, 'roc_auc_score': 0.5823890459475366}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
66,81.45201224088669,0:00:05,10.185427486896515,0:00:00,"{'f1': 0.2835913312693499, 'precision': 0.40316901408450706, 'accuracy': 0.7368061874431301, 'roc_auc_score': 0.5587479533869893}",22.783916383981705,"{'f1': 0.3220209237050268, 'precision': 0.42434431741761935, 'accuracy': 0.7409573949497904, 'roc_auc_score': 0.5750321327980494}",0:00:01,20211230_1739,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
67,81.15259730815887,0:00:05,10.143593609333038,0:00:00,"{'f1': 0.27822832189644414, 'precision': 0.4010791366906475, 'accuracy': 0.7368061874431301, 'roc_auc_score': 0.5567784136620919}",23.045610904693604,"{'f1': 0.31760763083268884, 'precision': 0.4257083621285418, 'accuracy': 0.7419323388905138, 'roc_auc_score': 0.5735456953085589}",0:00:01,20211230_1740,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
68,80.91554176807404,0:00:05,10.233673572540283,0:00:00,"{'f1': 0.283385579937304, 'precision': 0.4124087591240876, 'accuracy': 0.7399909008189263, 'roc_auc_score': 0.5598533596965323}",22.788148373365402,"{'f1': 0.32336255801959773, 'precision': 0.4336099585062241, 'accuracy': 0.7441747099541777, 'roc_auc_score': 0.576573981629393}",0:00:01,20211230_1740,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
69,80.82141423225403,0:00:05,10.157518923282623,0:00:00,"{'f1': 0.283385579937304, 'precision': 0.4124087591240876, 'accuracy': 0.7399909008189263, 'roc_auc_score': 0.5598533596965323}",22.68202105164528,"{'f1': 0.3222308288148722, 'precision': 0.43303261623872313, 'accuracy': 0.7440772155601053, 'roc_auc_score': 0.57608500084076}",0:00:01,20211230_1740,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
70,80.63882970809937,0:00:05,10.20631867647171,0:00:00,"{'f1': 0.2896379525593009, 'precision': 0.418018018018018, 'accuracy': 0.7411282984531392, 'roc_auc_score': 0.5625693909114269}",22.932091027498245,"{'f1': 0.3299075025693731, 'precision': 0.4397260273972603, 'accuracy': 0.7457346202593351, 'roc_auc_score': 0.5797217609719185}",0:00:01,20211230_1740,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
71,80.59530651569366,0:00:05,10.21586400270462,0:00:00,"{'f1': 0.29018136335209505, 'precision': 0.42028985507246375, 'accuracy': 0.741810737033667, 'roc_auc_score': 0.5630172858054251}",22.72118389606476,"{'f1': 0.32990221307256823, 'precision': 0.4408528198074278, 'accuracy': 0.7461245978356245, 'roc_auc_score': 0.5798356576845468}",0:00:01,20211230_1740,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
72,80.51612669229507,0:00:05,10.113250494003296,0:00:00,"{'f1': 0.28571428571428575, 'precision': 0.4188191881918819, 'accuracy': 0.741810737033667, 'roc_auc_score': 0.5613760027013439}",22.68214988708496,"{'f1': 0.32971576227390187, 'precision': 0.44367176634214184, 'accuracy': 0.7470995417763479, 'roc_auc_score': 0.5800495522952749}",0:00:01,20211230_1740,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
73,80.31608211994171,0:00:05,10.229692816734314,0:00:00,"{'f1': 0.2864321608040201, 'precision': 0.41834862385321103, 'accuracy': 0.7415832575068244, 'roc_auc_score': 0.5615549610241607}",22.576844036579132,"{'f1': 0.3305785123966942, 'precision': 0.4444444444444444, 'accuracy': 0.7472945305644926, 'roc_auc_score': 0.5804607365058012}",0:00:01,20211230_1740,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
74,79.9687283039093,0:00:05,10.095773935317993,0:00:00,"{'f1': 0.29283489096573206, 'precision': 0.4211469534050179, 'accuracy': 0.741810737033667, 'roc_auc_score': 0.5640020556678739}",22.637655168771744,"{'f1': 0.3357326478149101, 'precision': 0.44787379972565156, 'accuracy': 0.7480744857170712, 'roc_auc_score': 0.5828139450563309}",0:00:01,20211230_1740,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
75,80.36660853028297,0:00:05,10.035128563642502,0:00:00,"{'f1': 0.28750784682988073, 'precision': 0.4194139194139194, 'accuracy': 0.741810737033667, 'roc_auc_score': 0.5620325159429763}",22.634648382663727,"{'f1': 0.33057423693740295, 'precision': 0.4456066945606695, 'accuracy': 0.7476845081407819, 'roc_auc_score': 0.5805746332184294}",0:00:01,20211230_1740,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
76,79.73699370026588,0:00:05,9.949957221746445,0:00:00,"{'f1': 0.28858218318695106, 'precision': 0.42047531992687387, 'accuracy': 0.7420382165605095, 'roc_auc_score': 0.562510070861792}",22.372234731912613,"{'f1': 0.3327305605786618, 'precision': 0.4475330090340514, 'accuracy': 0.7481719801111436, 'roc_auc_score': 0.5816025937447451}",0:00:01,20211230_1740,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
77,79.50272014737129,0:00:05,10.063511669635773,0:00:00,"{'f1': 0.2869729389553178, 'precision': 0.42066420664206644, 'accuracy': 0.7422656960873522, 'roc_auc_score': 0.5620028559181589}",22.409254878759384,"{'f1': 0.3305699481865285, 'precision': 0.44677871148459386, 'accuracy': 0.7480744857170712, 'roc_auc_score': 0.5806885299310577}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
78,79.83098977804184,0:00:05,10.10430097579956,0:00:00,"{'f1': 0.28930817610062887, 'precision': 0.42357274401473294, 'accuracy': 0.7429481346678799, 'roc_auc_score': 0.5631072640537896}",22.315731674432755,"{'f1': 0.3326435592343508, 'precision': 0.44839609483960946, 'accuracy': 0.7484644632933606, 'roc_auc_score': 0.5816525926937952}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
79,79.93358772993088,0:00:05,9.979739934206009,0:00:00,"{'f1': 0.28967254408060455, 'precision': 0.42513863216266173, 'accuracy': 0.7434030937215651, 'roc_auc_score': 0.5634058606497884}",22.303172677755356,"{'f1': 0.3301323643913833, 'precision': 0.44757213230119636, 'accuracy': 0.7483669688992883, 'roc_auc_score': 0.5805968345384227}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
80,79.80594152212143,0:00:05,10.054369926452637,0:00:00,"{'f1': 0.289490245437382, 'precision': 0.42435424354243545, 'accuracy': 0.7431756141947224, 'roc_auc_score': 0.563256562351789}",22.492738366127014,"{'f1': 0.3318641431164117, 'precision': 0.44912280701754387, 'accuracy': 0.7487569464755777, 'roc_auc_score': 0.5814192029594755}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
81,79.52980309724808,0:00:05,9.930418014526367,0:00:00,"{'f1': 0.28769716088328073, 'precision': 0.42379182156133827, 'accuracy': 0.7431756141947224, 'roc_auc_score': 0.5626000491101565}",22.26931729912758,"{'f1': 0.33065144043602385, 'precision': 0.4482758620689655, 'accuracy': 0.7485619576874329, 'roc_auc_score': 0.5808663244072642}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
82,79.18376922607422,0:00:05,10.033790707588196,0:00:00,"{'f1': 0.2889589905362776, 'precision': 0.4256505576208178, 'accuracy': 0.7436305732484076, 'roc_auc_score': 0.5632269023269715}",22.1656491458416,"{'f1': 0.3305613305613306, 'precision': 0.4491525423728814, 'accuracy': 0.74885444086965, 'roc_auc_score': 0.580916323356314}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
83,79.48395121097565,0:00:05,10.04438203573227,0:00:00,"{'f1': 0.2885948330182735, 'precision': 0.42407407407407405, 'accuracy': 0.7431756141947224, 'roc_auc_score': 0.5629283057309727}",22.44452413916588,"{'f1': 0.3296931877275091, 'precision': 0.44837340876944837, 'accuracy': 0.7486594520815053, 'roc_auc_score': 0.5805051391457878}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
84,79.20532810688019,0:00:05,9.957259774208069,0:00:00,"{'f1': 0.2903834066624765, 'precision': 0.42463235294117646, 'accuracy': 0.7431756141947224, 'roc_auc_score': 0.5635848189726053}",22.37889128923416,"{'f1': 0.3304754481683554, 'precision': 0.4488355681016232, 'accuracy': 0.7487569464755777, 'roc_auc_score': 0.5808524255927358}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
85,79.79816129803658,0:00:05,9.92483165860176,0:00:00,"{'f1': 0.28589580686149935, 'precision': 0.4269449715370019, 'accuracy': 0.7443130118289354, 'roc_auc_score': 0.5623617707377048}",22.355712801218033,"{'f1': 0.3222251377591184, 'precision': 0.4452501812907904, 'accuracy': 0.7481719801111436, 'roc_auc_score': 0.5773517634941988}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
86,79.06842762231827,0:00:05,9.922187328338623,0:00:00,"{'f1': 0.29345088161209065, 'precision': 0.43068391866913125, 'accuracy': 0.7447679708826206, 'roc_auc_score': 0.5652864203002337}",22.23096203804016,"{'f1': 0.33523611831863004, 'precision': 0.4542897327707454, 'accuracy': 0.7502193623866628, 'roc_auc_score': 0.5832278354632588}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
87,79.46863651275635,0:00:05,9.867007166147232,0:00:00,"{'f1': 0.2895238095238095, 'precision': 0.4318181818181818, 'accuracy': 0.7454504094631483, 'roc_auc_score': 0.5640930320901505}",22.268439292907715,"{'f1': 0.32958997127187256, 'precision': 0.4516821760916249, 'accuracy': 0.7497318904163011, 'roc_auc_score': 0.5807829315200942}",0:00:01,20211230_1741,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
88,79.11573618650436,0:00:05,9.980900228023529,0:00:00,"{'f1': 0.29166666666666663, 'precision': 0.4301675977653631, 'accuracy': 0.7447679708826206, 'roc_auc_score': 0.564629907058601}",22.117485523223877,"{'f1': 0.3342864569794646, 'precision': 0.45441696113074204, 'accuracy': 0.7503168567807351, 'roc_auc_score': 0.5828666502017824}",0:00:01,20211230_1742,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
89,78.9909473657608,0:00:05,10.070301234722137,0:00:00,"{'f1': 0.290955091714105, 'precision': 0.4307116104868914, 'accuracy': 0.7449954504094631, 'roc_auc_score': 0.5644509487357843}",22.20692902803421,"{'f1': 0.3308113749021654, 'precision': 0.45253390435403285, 'accuracy': 0.7499268792044458, 'roc_auc_score': 0.5813358100723054}",0:00:01,20211230_1742,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
90,79.32360076904297,0:00:05,9.90444278717041,0:00:00,"{'f1': 0.289171974522293, 'precision': 0.4340344168260038, 'accuracy': 0.7461328480436761, 'roc_auc_score': 0.5642126703633326}",22.22573071718216,"{'f1': 0.32827225130890053, 'precision': 0.45172910662824206, 'accuracy': 0.7498293848103734, 'roc_auc_score': 0.5802800519169329}",0:00:01,20211230_1742,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
91,79.2163867354393,0:00:05,9.981515020132065,0:00:00,"{'f1': 0.2900763358778626, 'precision': 0.4342857142857143, 'accuracy': 0.7461328480436761, 'roc_auc_score': 0.5645409269841487}",22.289350360631943,"{'f1': 0.32888190625818275, 'precision': 0.4527757750540735, 'accuracy': 0.7501218679925904, 'roc_auc_score': 0.5806134395493526}",0:00:01,20211230_1742,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
92,78.94093307852745,0:00:05,9.847889482975006,0:00:00,"{'f1': 0.2913236225459151, 'precision': 0.4323308270676692, 'accuracy': 0.7454504094631483, 'roc_auc_score': 0.5647495453317831}",22.10299849510193,"{'f1': 0.3319393622582332, 'precision': 0.45552367288378764, 'accuracy': 0.7508043287510968, 'roc_auc_score': 0.5820525842861948}",0:00:01,20211230_1742,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
93,79.46835362911224,0:00:05,9.98373281955719,0:00:00,"{'f1': 0.2900763358778626, 'precision': 0.4342857142857143, 'accuracy': 0.7461328480436761, 'roc_auc_score': 0.5645409269841487}",22.225789189338684,"{'f1': 0.32992930086410055, 'precision': 0.4542177361211247, 'accuracy': 0.7505118455688798, 'roc_auc_score': 0.5811524192870354}",0:00:01,20211230_1742,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
94,78.9764775633812,0:00:05,9.935077160596848,0:00:00,"{'f1': 0.28898790579248884, 'precision': 0.43320610687022904, 'accuracy': 0.7459053685168335, 'roc_auc_score': 0.564063372065333}",22.286965668201447,"{'f1': 0.32949188056574125, 'precision': 0.4538239538239538, 'accuracy': 0.7504143511748075, 'roc_auc_score': 0.5809468271817723}",0:00:01,20211230_1742,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
95,78.95984655618668,0:00:05,9.970936447381973,0:00:00,"{'f1': 0.28898790579248884, 'precision': 0.43320610687022904, 'accuracy': 0.7459053685168335, 'roc_auc_score': 0.564063372065333}",22.093044936656952,"{'f1': 0.32949188056574125, 'precision': 0.4538239538239538, 'accuracy': 0.7504143511748075, 'roc_auc_score': 0.5809468271817723}",0:00:01,20211230_1742,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
96,79.0072226524353,0:00:05,10.150350749492645,0:00:00,"{'f1': 0.28898790579248884, 'precision': 0.43320610687022904, 'accuracy': 0.7459053685168335, 'roc_auc_score': 0.564063372065333}",22.018115401268005,"{'f1': 0.32817824377457405, 'precision': 0.45263919016630516, 'accuracy': 0.7501218679925904, 'roc_auc_score': 0.5803300508659829}",0:00:01,20211230_1742,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
97,79.20495986938477,0:00:06,9.928784132003784,0:00:00,"{'f1': 0.2869897959183673, 'precision': 0.43186180422264875, 'accuracy': 0.7456778889899909, 'roc_auc_score': 0.5632575605257011}",22.14488124847412,"{'f1': 0.3271199789971121, 'precision': 0.4524328249818446, 'accuracy': 0.7501218679925904, 'roc_auc_score': 0.5799049678409282}",0:00:01,20211230_1742,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
98,79.03144043684006,0:00:05,9.955650568008423,0:00:00,"{'f1': 0.2880815806246016, 'precision': 0.4329501915708812, 'accuracy': 0.7459053685168335, 'roc_auc_score': 0.5637351154445168}",22.1713604927063,"{'f1': 0.3271199789971121, 'precision': 0.4524328249818446, 'accuracy': 0.7501218679925904, 'roc_auc_score': 0.5799049678409282}",0:00:01,20211230_1743,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
99,78.9944960474968,0:00:05,9.966553002595901,0:00:00,"{'f1': 0.2878980891719745, 'precision': 0.4321223709369025, 'accuracy': 0.7456778889899909, 'roc_auc_score': 0.5635858171465173}",22.103268563747406,"{'f1': 0.3286163522012579, 'precision': 0.45303468208092484, 'accuracy': 0.7502193623866628, 'roc_auc_score': 0.5805356429712459}",0:00:01,20211230_1743,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
100,78.9548844397068,0:00:05,9.911558032035828,0:00:00,"{'f1': 0.2878980891719745, 'precision': 0.4321223709369025, 'accuracy': 0.7456778889899909, 'roc_auc_score': 0.5635858171465173}",22.299439817667007,"{'f1': 0.32817824377457405, 'precision': 0.45263919016630516, 'accuracy': 0.7501218679925904, 'roc_auc_score': 0.5803300508659829}",0:00:01,20211230_1743,/Users/barryzhou/Desktop/Barry2022/codes/ltr/ltr_sota/examples/adult_widedeep/results/Model_LR2e-05_Batch256_LossBCEWithLogitsLoss
